{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the the use of computer vision to assist in creating metadata for the Tennessee Farm News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import IntProgress, Label, VBox\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "import img_qc.img_qc as img_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_percent_of_image(image, percentage):\n",
    "    width, height = image.size\n",
    "    box = (0, 0, width, int(height * percentage))  # percentage as float\n",
    "    image_cropped = image.crop(box)\n",
    "    return image_cropped\n",
    "\n",
    "def if_rgb_convert_to_gray(np_image):\n",
    "    if len(np_image.shape) > 2:\n",
    "        np_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = Path('/Volumes/fluffy/ProjectCeres/00_for_CRL/agrtfn')\n",
    "\n",
    "page_1_paths_list = sorted(data_dir_path.glob('**/*_0001.tif'))\n",
    "# remove macOS '.' index files\n",
    "page_1_paths_list = [x for x in page_1_paths_list if not str(x.stem).startswith('.')]\n",
    "len(page_1_paths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is processing all issues in {data_dir_path} from the Tennessee Farm News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_grayscale = 0\n",
    "number_of_rgb = 0\n",
    "gray_image_paths_list = []\n",
    "rgb_image_paths_list = []\n",
    "for image_path in page_1_paths_list:\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode == 'L':\n",
    "        number_of_grayscale += 1\n",
    "        gray_image_paths_list.append(image_path)\n",
    "    elif image.mode == 'RGB':\n",
    "        number_of_rgb += 1\n",
    "        rgb_image_paths_list.append(image_path)\n",
    "print(f'# of grayscale: {number_of_grayscale}')\n",
    "print(f'      # of rgb: {number_of_rgb}')\n",
    "print(f'  total images: {number_of_grayscale + number_of_rgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# let's look at the top 1/3 of the image of all the rgb images since there aren't that many\n",
    "for image_path in rgb_image_paths_list:\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    box = (0, 0, width, int(height/3))  # (left, upper, right, lower)\n",
    "    image_cropped = image.crop(box)\n",
    "    plt.imshow(image_cropped)\n",
    "    plt.title(image_path.stem)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the following titles have a page 1 in color:\n",
    "* Agricultural & home economics news\n",
    "* Agricultural and home economics packet\n",
    "* Agricultural news\n",
    "\n",
    "Furthermore, the amount of each color on the page is very different in each title, so an average image color may be enough to identify each title. The PROBLEM, and it's a pain, is that there are 2 versions of the Agricultural & Home Economic News, which means I need 2 versions of that one to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get sample image from Agricultural & home economic news\n",
    "ag_home_ec_news_yellow_path = [x for x in rgb_image_paths_list if x.stem.endswith('002750_0001')][0]\n",
    "ag_home_ec_news_yellow = Image.open(ag_home_ec_news_yellow_path)\n",
    "ag_home_ec_news_yellow = get_top_percent_of_image(ag_home_ec_news_yellow, 0.3)\n",
    "plt.imshow(ag_home_ec_news_yellow)\n",
    "plt.title(ag_home_ec_news_yellow_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1100, 50, 3150, 900)  # (left, upper, right, lower)\n",
    "ag_home_ec_news_yellow_title_crop = ag_home_ec_news_yellow.crop(box)\n",
    "plt.imshow(ag_home_ec_news_yellow_title_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get sample image from Agricultural & home economic news\n",
    "ag_home_ec_news_orange_path = [x for x in rgb_image_paths_list if x.stem.endswith('002951_0001')][0]\n",
    "ag_home_ec_news_orange = Image.open(ag_home_ec_news_orange_path)\n",
    "ag_home_ec_news_orange = get_top_percent_of_image(ag_home_ec_news_orange, 0.3)\n",
    "plt.imshow(ag_home_ec_news_orange)\n",
    "plt.title(ag_home_ec_news_orange_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (250, 500, 3050, 1100)  # (left, upper, right, lower)\n",
    "ag_home_ec_news_orange_title_crop = ag_home_ec_news_orange.crop(box)\n",
    "plt.imshow(ag_home_ec_news_orange_title_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get sample image from Agricultural news\n",
    "ag_news_path = [x for x in rgb_image_paths_list if x.stem.endswith('003446_0001')][0]\n",
    "ag_news = Image.open(ag_news_path)\n",
    "ag_news = get_top_percent_of_image(ag_news, 0.3)\n",
    "plt.imshow(ag_news)\n",
    "plt.title(ag_news_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1150, 50, 3200, 850)  # (left, upper, right, lower)\n",
    "ag_news_title_crop = ag_news.crop(box)\n",
    "plt.imshow(ag_news_title_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get sample image from Agricultural and home economic packet\n",
    "ag_home_ec_packet_path = [x for x in rgb_image_paths_list if x.stem.endswith('003295_0001')][0]\n",
    "ag_home_ec_packet = Image.open(ag_home_ec_packet_path)\n",
    "ag_home_ec_packet = get_top_percent_of_image(ag_home_ec_packet, 0.3)\n",
    "plt.imshow(ag_home_ec_packet)\n",
    "plt.title(ag_home_ec_packet_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (250, 450, 1450, 1150)  # (left, upper, right, lower)\n",
    "ag_home_ec_packet_title_crop = ag_home_ec_packet.crop(box)\n",
    "plt.imshow(ag_home_ec_packet_title_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm and home news\n",
    "tn_farm_home_news_path = [x for x in rgb_image_paths_list if x.stem.endswith('001677_0001')][0]\n",
    "tn_farm_home_news = Image.open(tn_farm_home_news_path)\n",
    "tn_farm_home_news = get_top_percent_of_image(tn_farm_home_news, 0.3)\n",
    "plt.imshow(tn_farm_home_news)\n",
    "plt.title(tn_farm_home_news_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1400, 100, 3150, 250)  # (left, upper, right, lower)\n",
    "tn_farm_home_news_title_crop = tn_farm_home_news.crop(box)\n",
    "plt.imshow(tn_farm_home_news_title_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm and home news\n",
    "tn_farm_home_news_gray_path = [x for x in page_1_paths_list if x.stem.endswith('002145_0001')][0]\n",
    "tn_farm_home_news_gray = Image.open(tn_farm_home_news_gray_path)\n",
    "tn_farm_home_news_gray = get_top_percent_of_image(tn_farm_home_news_gray, 0.3)\n",
    "plt.imshow(tn_farm_home_news_gray, cmap='gray')\n",
    "plt.title(tn_farm_home_news_gray_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1450, 175, 3175, 325)  # (left, upper, right, lower)\n",
    "tn_farm_home_news_gray_title_crop = tn_farm_home_news_gray.crop(box)\n",
    "plt.imshow(tn_farm_home_news_gray_title_crop, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm and home news\n",
    "tn_farm_home_news_gray_early_path = [x for x in page_1_paths_list if x.stem.endswith('000964_0001')][0]\n",
    "tn_farm_home_news_gray_early = Image.open(tn_farm_home_news_gray_early_path)\n",
    "tn_farm_home_news_gray_early = get_top_percent_of_image(tn_farm_home_news_gray_early, 0.3)\n",
    "plt.imshow(tn_farm_home_news_gray_early, cmap='gray')\n",
    "plt.title(tn_farm_home_news_gray_early_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1400, 150, 3175, 290)  # (left, upper, right, lower)\n",
    "tn_farm_home_news_gray_early_title_crop = tn_farm_home_news_gray_early.crop(box)\n",
    "plt.imshow(tn_farm_home_news_gray_early_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Farm news\n",
    "farm_news_path = [x for x in page_1_paths_list if x.stem.endswith('002357_0001')][0]\n",
    "farm_news = Image.open(farm_news_path)\n",
    "farm_news = get_top_percent_of_image(farm_news, 0.3)\n",
    "plt.imshow(farm_news, cmap='gray')\n",
    "plt.title(farm_news_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1400, 450, 2650, 800)  # (left, upper, right, lower)\n",
    "farm_news_title_crop = farm_news.crop(box)\n",
    "plt.imshow(farm_news_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm news\n",
    "tn_farm_news_path = [x for x in page_1_paths_list if x.stem.endswith('000004_0001')][0]\n",
    "tn_farm_news = Image.open(tn_farm_news_path)\n",
    "tn_farm_news = get_top_percent_of_image(tn_farm_news, 0.3)\n",
    "plt.imshow(tn_farm_news, cmap='gray')\n",
    "plt.title(tn_farm_news_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (850, 500, 2150, 750)  # (left, upper, right, lower)\n",
    "tn_farm_news_title_crop = tn_farm_news.crop(box)\n",
    "plt.imshow(tn_farm_news_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm news\n",
    "tn_farm_news2_path = [x for x in page_1_paths_list if x.stem.endswith('000015_0001')][0]\n",
    "tn_farm_news2 = Image.open(tn_farm_news2_path)\n",
    "tn_farm_news2 = get_top_percent_of_image(tn_farm_news2, 0.3)\n",
    "plt.imshow(tn_farm_news2, cmap='gray')\n",
    "plt.title(tn_farm_news2_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (550, 600, 1950, 750)  # (left, upper, right, lower)\n",
    "tn_farm_news2_title_crop = tn_farm_news2.crop(box)\n",
    "plt.imshow(tn_farm_news2_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm news\n",
    "tn_farm_news3_path = [x for x in page_1_paths_list if x.stem.endswith('000456_0001')][0]\n",
    "tn_farm_news3 = Image.open(tn_farm_news3_path)\n",
    "tn_farm_news3 = get_top_percent_of_image(tn_farm_news3, 0.3)\n",
    "plt.imshow(tn_farm_news3, cmap='gray')\n",
    "plt.title(tn_farm_news3_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1000, 550, 2400, 675)  # (left, upper, right, lower)\n",
    "tn_farm_news3_title_crop = tn_farm_news3.crop(box)\n",
    "plt.imshow(tn_farm_news3_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm news\n",
    "tn_farm_news4_path = [x for x in page_1_paths_list if x.stem.endswith('000250_0001')][0]\n",
    "tn_farm_news4 = Image.open(tn_farm_news4_path)\n",
    "tn_farm_news4 = get_top_percent_of_image(tn_farm_news4, 0.3)\n",
    "plt.imshow(tn_farm_news4, cmap='gray')\n",
    "plt.title(tn_farm_news4_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (800, 450, 2200, 575)  # (left, upper, right, lower)\n",
    "tn_farm_news4_title_crop = tn_farm_news4.crop(box)\n",
    "plt.imshow(tn_farm_news4_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from Tennesse farm news\n",
    "tn_farm_news5_path = [x for x in page_1_paths_list if x.stem.endswith('000003_0001')][0]\n",
    "tn_farm_news5 = Image.open(tn_farm_news5_path)\n",
    "tn_farm_news5 = get_top_percent_of_image(tn_farm_news5, 0.3)\n",
    "plt.imshow(tn_farm_news5, cmap='gray')\n",
    "plt.title(tn_farm_news5_path.stem)\n",
    "plt.show()\n",
    "\n",
    "box = (1050, 600, 2250, 690)  # (left, upper, right, lower)\n",
    "tn_farm_news5_title_crop = tn_farm_news5.crop(box)\n",
    "plt.imshow(tn_farm_news5_title_crop, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# try to match cropped title to scanned page\n",
    "MIN_MATCH_COUNT = 4\n",
    "\n",
    "image_search_for = np.array(ag_home_ec_packet_title_crop)\n",
    "image_look_in = np.array(ag_home_ec_packet)\n",
    "\n",
    "# grayscale images\n",
    "image_search_for_gray = cv2.cvtColor(image_search_for, cv2.COLOR_RGB2GRAY)\n",
    "image_look_in_gray = cv2.cvtColor(image_look_in, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "## (2) Create SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "## (3) Create flann matcher\n",
    "matcher = cv2.FlannBasedMatcher(dict(algorithm = 1, trees = 5), {})\n",
    "\n",
    "## (4) Detect keypoints and compute keypointer descriptors\n",
    "kpts1, descs1 = sift.detectAndCompute(image_search_for_gray,None)\n",
    "kpts2, descs2 = sift.detectAndCompute(image_look_in_gray,None)\n",
    "\n",
    "## (5) knnMatch to get Top2\n",
    "matches = matcher.knnMatch(descs1, descs2, 2)\n",
    "# Sort by their distance.\n",
    "matches = sorted(matches, key = lambda x:x[0].distance)\n",
    "\n",
    "## (6) Ratio test, to get good matches.\n",
    "good = [m1 for (m1, m2) in matches if m1.distance < 0.7 * m2.distance]\n",
    "\n",
    "canvas = image_look_in.copy()\n",
    "\n",
    "## (7) find homography matrix\n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    ## (queryIndex for the small object, trainIndex for the scene )\n",
    "    src_pts = np.float32([ kpts1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kpts2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    ## find homography matrix in cv2.RANSAC using good match points\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    h,w = image_search_for.shape[:2]\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    cv2.polylines(canvas,[np.int32(dst)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "else:\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good),MIN_MATCH_COUNT))\n",
    "\n",
    "\n",
    "## (8) drawMatches\n",
    "matched = cv2.drawMatches(image_search_for,kpts1,canvas,kpts2,good,None)#,**draw_params)\n",
    "\n",
    "## (9) Crop the matched region from scene\n",
    "h,w = image_search_for.shape[:2]\n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "dst = cv2.perspectiveTransform(pts,M)\n",
    "perspectiveM = cv2.getPerspectiveTransform(np.float32(dst),pts)\n",
    "found = cv2.warpPerspective(image_look_in,perspectiveM,(w,h))\n",
    "\n",
    "# display images\n",
    "plt.imshow(matched)\n",
    "plt.title('Matched')\n",
    "plt.show()\n",
    "plt.imshow(found)\n",
    "plt.title('Found')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load 2nd Ag home ec packet image to try and find title in\n",
    "ag_home_ec_packet_test_path = [x for x in rgb_image_paths_list if x.stem.endswith('003284_0001')][0]\n",
    "ag_home_ec_packet_test = Image.open(ag_home_ec_packet_test_path)\n",
    "ag_home_ec_packet_test = get_top_percent_of_image(ag_home_ec_packet_test, 0.3)\n",
    "plt.imshow(ag_home_ec_packet_test)\n",
    "plt.title(ag_home_ec_packet_test_path.stem)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# try to match cropped title to a DIFFERENT scanned page!\n",
    "MIN_MATCH_COUNT = 4\n",
    "\n",
    "image_search_for = np.array(ag_home_ec_packet_title_crop)\n",
    "image_look_in = np.array(ag_home_ec_packet_test)\n",
    "\n",
    "# grayscale images\n",
    "image_search_for_gray = cv2.cvtColor(image_search_for, cv2.COLOR_RGB2GRAY)\n",
    "image_look_in_gray = cv2.cvtColor(image_look_in, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "## (2) Create SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "## (3) Create flann matcher\n",
    "matcher = cv2.FlannBasedMatcher(dict(algorithm = 1, trees = 5), {})\n",
    "\n",
    "## (4) Detect keypoints and compute keypointer descriptors\n",
    "kpts1, descs1 = sift.detectAndCompute(image_search_for_gray,None)\n",
    "kpts2, descs2 = sift.detectAndCompute(image_look_in_gray,None)\n",
    "\n",
    "## (5) knnMatch to get Top2\n",
    "matches = matcher.knnMatch(descs1, descs2, 2)\n",
    "# Sort by their distance.\n",
    "matches = sorted(matches, key = lambda x:x[0].distance)\n",
    "\n",
    "## (6) Ratio test, to get good matches.\n",
    "good = [m1 for (m1, m2) in matches if m1.distance < 0.7 * m2.distance]\n",
    "\n",
    "canvas = image_look_in.copy()\n",
    "\n",
    "## (7) find homography matrix\n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    ## (queryIndex for the small object, trainIndex for the scene )\n",
    "    src_pts = np.float32([ kpts1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kpts2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    ## find homography matrix in cv2.RANSAC using good match points\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    h,w = image_search_for.shape[:2]\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    cv2.polylines(canvas,[np.int32(dst)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "else:\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good),MIN_MATCH_COUNT))\n",
    "\n",
    "\n",
    "## (8) drawMatches\n",
    "matched = cv2.drawMatches(image_search_for,kpts1,canvas,kpts2,good,None)#,**draw_params)\n",
    "\n",
    "## (9) Crop the matched region from scene\n",
    "h,w = image_search_for.shape[:2]\n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "dst = cv2.perspectiveTransform(pts,M)\n",
    "perspectiveM = cv2.getPerspectiveTransform(np.float32(dst),pts)\n",
    "found = cv2.warpPerspective(image_look_in,perspectiveM,(w,h))\n",
    "\n",
    "# display images\n",
    "plt.imshow(matched)\n",
    "plt.title('Matched')\n",
    "plt.show()\n",
    "plt.imshow(found)\n",
    "plt.title('Found')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def old_find_crop(image_search_for, image_look_in, minimum_matches, distance_ratio):\n",
    "\n",
    "    image_search_for = np.array(image_search_for)\n",
    "    image_look_in = np.array(image_look_in)\n",
    "\n",
    "    # convert to grayscale if necessary\n",
    "    image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "    \n",
    "    # equalize histogram of image we're looking in (already done for title crop)\n",
    "    # image_look_in_gray = cv2.equalizeHist(image_look_in_gray)\n",
    "    # image_search_for_gray = cv2.equalizeHist(image_search_for_gray)\n",
    "\n",
    "    ## (2) Create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    ## (3) Create flann matcher\n",
    "    matcher = cv2.FlannBasedMatcher(dict(algorithm = 1, trees = 5), {})\n",
    "\n",
    "    ## (4) Detect keypoints and compute keypointer descriptors\n",
    "    kpts1, descs1 = sift.detectAndCompute(image_search_for_gray,None)\n",
    "    kpts2, descs2 = sift.detectAndCompute(image_look_in_gray,None)\n",
    "\n",
    "    ## (5) knnMatch to get Top2\n",
    "    matches = matcher.knnMatch(descs1, descs2, 2)\n",
    "    # Sort by their distance.\n",
    "    matches = sorted(matches, key = lambda x:x[0].distance)\n",
    "\n",
    "    ## (6) Ratio test, to get good matches.\n",
    "    # distance_ratio was originally 0.7\n",
    "    good = [m1 for (m1, m2) in matches if m1.distance < distance_ratio * m2.distance]\n",
    "    # print(f'                   matches: {len(good)}')\n",
    "\n",
    "    canvas = image_look_in.copy()\n",
    "\n",
    "    ## (7) find homography matrix\n",
    "    if len(good)>minimum_matches:\n",
    "        ## (queryIndex for the small object, trainIndex for the scene )\n",
    "        src_pts = np.float32([ kpts1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kpts2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        ## find homography matrix in cv2.RANSAC using good match points\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        if M is not None:\n",
    "            h,w = image_search_for.shape[:2]\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "            cv2.polylines(canvas,[np.int32(dst)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "\n",
    "            ## (8) drawMatches\n",
    "            matched = cv2.drawMatches(image_search_for,kpts1,canvas,kpts2,good,None)#,**draw_params)\n",
    "\n",
    "            ## (9) Crop the matched region from scene\n",
    "            h,w = image_search_for.shape[:2]\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "            perspectiveM = cv2.getPerspectiveTransform(np.float32(dst),pts)\n",
    "            found = cv2.warpPerspective(image_look_in,perspectiveM,(w,h))\n",
    "\n",
    "            # display images\n",
    "            # fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "            # axes[0].imshow(matched)\n",
    "            # axes[0].set_title(f'Matched: {len(good)}')\n",
    "            # axes[1].imshow(found)\n",
    "            # axes[1].set_title('Found')\n",
    "            # plt.show()\n",
    "\n",
    "            return len(good), matched, found\n",
    "        \n",
    "    \n",
    "        # print('')\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print( \"Not enough matches are found - {}/{}\".format(len(good),minimum_matches))\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print('')\n",
    "              \n",
    "    return len(good), None, None  # return 0 for images if there weren't enough matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html\n",
    "def find_crop(image_search_for, image_look_in, minimum_matches, distance_ratio):\n",
    "\n",
    "    image_search_for = np.array(image_search_for)\n",
    "    image_look_in = np.array(image_look_in)\n",
    "\n",
    "    # convert to grayscale if necessary\n",
    "    image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "    \n",
    "    # equalize histogram of image we're looking in (already done for title crop)\n",
    "    # image_look_in_gray = cv2.equalizeHist(image_look_in_gray)\n",
    "    # image_search_for_gray = cv2.equalizeHist(image_search_for_gray)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    # find keypoints and descriptors with SIFT\n",
    "    keypoints_1, descriptors_1 = sift.detectAndCompute(image_search_for_gray, None)\n",
    "    keypoints_2, descriptors_2 = sift.detectAndCompute(image_look_in_gray, None)\n",
    "    \n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_parameters = dict(checks = 50)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "    \n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "    \n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "    # print(f'before: {number_of_good_matches}')\n",
    "            \n",
    "    if number_of_good_matches >= minimum_matches:\n",
    "        source_points = np.float32([ keypoints_1[m.queryIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "        destination_points = np.float32([ keypoints_2[m.trainIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "        \n",
    "        matrix, mask = cv2.findHomography(source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "        if matrix is not None:\n",
    "            \n",
    "            # matches_mask = mask.ravel().tolist()\n",
    "            \n",
    "            keypoints_2, descriptors_2 = sift.detectAndCompute(image_look_in_gray, mask)\n",
    "            matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "            good_matches = []\n",
    "            for m, n in matches:\n",
    "                if m.distance < distance_ratio * n.distance:\n",
    "                    good_matches.append(m)\n",
    "            number_of_good_matches = len(good_matches)\n",
    "            if number_of_good_matches <= minimum_matches:\n",
    "                pass\n",
    "            else:\n",
    "                source_points = np.float32([ keypoints_1[m.queryIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "                destination_points = np.float32([ keypoints_2[m.trainIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "                matrix, mask = cv2.findHomography(source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "                matches_mask = mask.ravel().tolist()\n",
    "\n",
    "                height, width = image_search_for_gray.shape\n",
    "                points = np.float32([ [0,0], [0,height-1], [width-1,height-1], [width-1,0] ]).reshape(-1, 1, 2)\n",
    "                print(number_of_good_matches)\n",
    "                destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "                canvas = image_look_in.copy()\n",
    "                canvas = cv2.polylines(canvas, [np.int32(destination)], True, (255, 0, 0), 10, cv2.LINE_AA)\n",
    "\n",
    "                # draw matches\n",
    "                draw_parameters = dict(\n",
    "                    matchColor = (0, 255, 0),\n",
    "                    singlePointColor = None,\n",
    "                    matchesMask = matches_mask,  # only draw inliers\n",
    "                    flags = 2  # don't draw single keypoints\n",
    "                )\n",
    "\n",
    "                matched_graphic = cv2.drawMatches(\n",
    "                    image_search_for,\n",
    "                    keypoints_1,\n",
    "                    canvas,\n",
    "                    keypoints_2,\n",
    "                    good_matches,\n",
    "                    None,\n",
    "                    **draw_parameters\n",
    "                )\n",
    "\n",
    "                # crop found image\n",
    "                perspective_matrix = cv2.getPerspectiveTransform(np.float32(destination), points)\n",
    "                found_image = cv2.warpPerspective(image_look_in, perspective_matrix, (width, height))\n",
    "\n",
    "                return number_of_good_matches, matched_graphic, found_image\n",
    "        \n",
    "    \n",
    "        # print('')\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print( \"Not enough matches are found - {}/{}\".format(len(good),minimum_matches))\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print('')\n",
    "              \n",
    "    return number_of_good_matches, None, None  # return 0 for images if there weren't enough matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html\n",
    "def find_crop(image_search_for, image_look_in, minimum_matches, distance_ratio):\n",
    "\n",
    "    image_search_for = np.array(image_search_for)\n",
    "    image_look_in = np.array(image_look_in)\n",
    "\n",
    "    # convert to grayscale if necessary\n",
    "    image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "    \n",
    "    # equalize histogram of image we're looking in (already done for title crop)\n",
    "    # image_look_in_gray = cv2.equalizeHist(image_look_in_gray)\n",
    "    # image_search_for_gray = cv2.equalizeHist(image_search_for_gray)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    # find keypoints and descriptors with SIFT\n",
    "    keypoints_1, descriptors_1 = sift.detectAndCompute(image_search_for_gray, None)\n",
    "    keypoints_2, descriptors_2 = sift.detectAndCompute(image_look_in_gray, None)\n",
    "    \n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_parameters = dict(checks = 50)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "    \n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "    \n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "    # print(f'before: {number_of_good_matches}')\n",
    "            \n",
    "    if number_of_good_matches >= minimum_matches:\n",
    "        source_points = np.float32([ keypoints_1[m.queryIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "        destination_points = np.float32([ keypoints_2[m.trainIdx].pt for m in good_matches ]).reshape(-1, 1, 2)\n",
    "        \n",
    "        matrix, mask = cv2.findHomography(source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "        if matrix is not None:\n",
    "            \n",
    "            matches_mask = mask.ravel().tolist()\n",
    "\n",
    "            height, width = image_search_for_gray.shape\n",
    "            points = np.float32([ [0,0], [0,height-1], [width-1,height-1], [width-1,0] ]).reshape(-1, 1, 2)\n",
    "            destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "            canvas = image_look_in.copy()\n",
    "            canvas = cv2.polylines(canvas, [np.int32(destination)], True, (255, 0, 0), 10, cv2.LINE_AA)\n",
    "\n",
    "            # draw matches\n",
    "            draw_parameters = dict(\n",
    "                matchColor = (0, 255, 0),\n",
    "                singlePointColor = None,\n",
    "                matchesMask = matches_mask,  # only draw inliers\n",
    "                flags = 2  # don't draw single keypoints\n",
    "            )\n",
    "\n",
    "            matched_graphic = cv2.drawMatches(\n",
    "                image_search_for,\n",
    "                keypoints_1,\n",
    "                canvas,\n",
    "                keypoints_2,\n",
    "                good_matches,\n",
    "                None,\n",
    "                **draw_parameters\n",
    "            )\n",
    "\n",
    "            # crop found image\n",
    "            perspective_matrix = cv2.getPerspectiveTransform(np.float32(destination), points)\n",
    "            found_image = cv2.warpPerspective(image_look_in, perspective_matrix, (width, height))\n",
    "\n",
    "            return number_of_good_matches, matched_graphic, found_image\n",
    "        \n",
    "    \n",
    "        # print('')\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print( \"Not enough matches are found - {}/{}\".format(len(good),minimum_matches))\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print('')\n",
    "              \n",
    "    return number_of_good_matches, None, None  # return 0 for images if there weren't enough matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get title crops\n",
    "\n",
    "# create dictionary of series titles and paths to a representative 1st page\n",
    "images_for_title_crops_dict = {\n",
    "    'Agricultural and home economics packet': ag_home_ec_packet_path,\n",
    "    'Agricultural & home economics news_orange': ag_home_ec_news_orange_path,\n",
    "    'Agricultural & home economics news_yellow': ag_home_ec_news_yellow_path,\n",
    "    'Agricultural news': ag_news_path,\n",
    "    'Tennessee farm and home news': tn_farm_home_news_path,\n",
    "    'Tennessee farm and home news_gray': tn_farm_home_news_gray_path,\n",
    "    'Tennessee farm and home news_gray_early': tn_farm_home_news_gray_early_path,\n",
    "    'Farm news': farm_news_path,\n",
    "    'Tennessee farm news': tn_farm_news_path,\n",
    "    'Tennessee farm news2': tn_farm_news2_path,\n",
    "    'Tennessee farm news3': tn_farm_news3_path,\n",
    "    'Tennessee farm news4': tn_farm_news4_path,\n",
    "    'Tennessee farm news5': tn_farm_news5_path,\n",
    "}\n",
    "\n",
    "# title crop boxes\n",
    "crop_boxes_dict = {\n",
    "    'Agricultural and home economics packet': (250, 450, 1450, 1150),\n",
    "    'Agricultural & home economics news_orange': (250, 500, 3050, 1100),\n",
    "    'Agricultural & home economics news_yellow': (1100, 50, 3150, 900),\n",
    "    'Agricultural news': (1150, 50, 3200, 850),\n",
    "    'Tennessee farm and home news': (1400, 100, 3150, 250),\n",
    "    'Tennessee farm and home news_gray': (1450, 175, 3175, 325),\n",
    "    'Tennessee farm and home news_gray_early': (1400, 150, 3175, 290),\n",
    "    'Farm news': (1400, 450, 2650, 800),\n",
    "    'Tennessee farm news': (850, 500, 2150, 750),\n",
    "    'Tennessee farm news2': (550, 600, 1950, 750),\n",
    "    'Tennessee farm news3': (1000, 550, 2400, 675),\n",
    "    'Tennessee farm news4': (800, 450, 2200, 575),\n",
    "    'Tennessee farm news5': (1050, 600, 2250, 690),\n",
    "}\n",
    "\n",
    "# instantiate dictionary for the images we're searching for, i.e. the titles\n",
    "title_crops_dict = {}\n",
    "bgr_title_crops_dict = {}\n",
    "gray_title_crops_dict = {}\n",
    "\n",
    "# loop through the dictionary titles\n",
    "for title in images_for_title_crops_dict:\n",
    "    \n",
    "    # get the image path value out of the dictionary using the looped title variable as a key\n",
    "    image_path = images_for_title_crops_dict[title]\n",
    "    \n",
    "    print(f'Processing {title} . . .')\n",
    "    \n",
    "    # convert to string and open as open numpy array with openCV\n",
    "    image_for_title_crop = cv2.imread(str(image_path))  # cv2.imread requires string\n",
    "    \n",
    "    # load crop box using title; originally cropped with PIL so convert to numpy values\n",
    "    x1, y1, x2, y2 = crop_boxes_dict[title]\n",
    "    \n",
    "    # crop image to title\n",
    "    title_crop = image_for_title_crop[y1:y2, x1:x2].copy()  # copy for better memory use\n",
    "    \n",
    "    image = Image.fromarray(title_crop)\n",
    "    print(image.mode)\n",
    "    \n",
    "    # add cropped image to dictionary\n",
    "    title_crops_dict.update( {title: title_crop} )\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode == 'L':  # then Grayscale\n",
    "        gray_title_crops_dict.update( {title: title_crop})\n",
    "        cmap = 'gray'\n",
    "    else: # then BGR\n",
    "        bgr_title_crops_dict.update( {title: title_crop})\n",
    "        title_crop = cv2.cvtColor(title_crop, cv2.COLOR_BGR2RGB)\n",
    "        cmap = None\n",
    "        \n",
    "    plt.imshow(title_crop, cmap=cmap)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "print('')\n",
    "print(f'{len(title_crops_dict)} items in dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify titles!\n",
    "\n",
    "gray_minimum_matches = 5\n",
    "gray_distance_ratio = 0.6\n",
    "bgr_minimum_matches = 40\n",
    "bgr_distance_ratio = 0.55\n",
    "\n",
    "image_paths_list = page_1_paths_list\n",
    "\n",
    "# progress bar\n",
    "progress_label = Label('Images to process')\n",
    "progress_bar = IntProgress(min=0, max=len(image_paths_list))\n",
    "progress_widget = VBox([progress_label, progress_bar])\n",
    "display(progress_widget)\n",
    "\n",
    "number_of_no_good_matches = 0\n",
    "number_found_match = 0\n",
    "\n",
    "for index, image_path in enumerate(image_paths_list, start=1):\n",
    "    \n",
    "    label = f'Processing {image_path.name} . . . \\r    {index}/{len(image_paths_list)}'\n",
    "    progress_label.value = label\n",
    "    \n",
    "    print(f'Begin: {image_path.stem}')\n",
    "    \n",
    "    # load image and crop to top 1/3\n",
    "    image = cv2.imread(str(image_path))\n",
    "    height, width = image.shape[:2]\n",
    "    image_cropped = image[0:(int(height/3)), 0:width].copy() # [y1:y2, x1:x2]\n",
    "    \n",
    "    # set variables\n",
    "    most_matches = 0\n",
    "    best_matched_image = 0\n",
    "    best_found_image = None\n",
    "    best_title = None\n",
    "    best_ssim_with_title = -1  # ssim varies from -1 to 1 (perfect match)\n",
    "    best_match_value = 0\n",
    "    \n",
    "    # check if image is bgr or grayscale to get title dict to use\n",
    "    with Image.open(image_path) as image:\n",
    "        if image.mode == 'L':  # then Grayscale\n",
    "            grayscale = True\n",
    "            title_dict = gray_title_crops_dict\n",
    "            minimum_matches = gray_minimum_matches\n",
    "            distance_ratio = gray_distance_ratio\n",
    "        else:\n",
    "            grayscale = False\n",
    "            title_dict = bgr_title_crops_dict\n",
    "            minimum_matches = bgr_minimum_matches\n",
    "            distance_ratio = bgr_distance_ratio\n",
    "    \n",
    "    for title in title_dict:\n",
    "        title_crop = title_dict[title]\n",
    "        \n",
    "        print(f'        testing for: {title}')\n",
    "        number_of_matches, matched_image, found_image = find_crop(title_crop, image_cropped, minimum_matches, distance_ratio)\n",
    "        print(f'                   matches/minimum matches: {number_of_matches}/{minimum_matches}')\n",
    "        \"\"\n",
    "        if number_of_matches > minimum_matches:\n",
    "            if found_image is not None:\n",
    "                # get the structural similarity index of the match with the image for a 2nd heuristic    \n",
    "                np_found = cv2.cvtColor(found_image, cv2.COLOR_BGR2GRAY)\n",
    "                np_title_crop = cv2.cvtColor(np.array(title_crop), cv2.COLOR_BGR2GRAY)\n",
    "                # print(f'np_found: {np_found.shape}')\n",
    "                # print(f'np_title_crop: {np_title_crop.shape}')\n",
    "                try:\n",
    "                    ssim_with_title = ssim(np_title_crop, np_found, multichannel=True)\n",
    "                    print(f'                   ssim: {ssim_with_title}')\n",
    "                    if ssim_with_title > best_ssim_with_title:\n",
    "                        best_ssim_with_title = ssim_with_title\n",
    "                        most_matches = number_of_matches\n",
    "                        best_matched_image = matched_image\n",
    "                        best_found_image = found_image\n",
    "                        best_title = title\n",
    "\n",
    "                except ValueError:  # image shape doesn't match, so most likely incorrect title\n",
    "                    continue\n",
    "\n",
    "                if best_title == 'Agricultural & home economics news_orange' or best_title == 'Agricultural & home economics news_yellow':\n",
    "                    best_title = 'Agricultural & home economics news'\n",
    "                elif best_title == 'Tennessee farm and home news_gray' or best_title == 'Tennessee farm and home news_gray_early':\n",
    "                    best_title = 'Tennessee farm and home news'\n",
    "\n",
    "    if isinstance(best_matched_image, int):\n",
    "        print('')\n",
    "        print('     ***** No good match found *****')\n",
    "        print('')\n",
    "        number_of_no_good_matches += 1\n",
    "        \n",
    "    else:\n",
    "        # convert to RGB channel order and show crop\n",
    "        if len(best_matched_image.shape) > 2:\n",
    "            best_matched_image = cv2.cvtColor(best_matched_image, cv2.COLOR_BGR2RGB)\n",
    "            best_found_image = cv2.cvtColor(best_found_image, cv2.COLOR_BGR2RGB)\n",
    "            cmap = None\n",
    "        else:\n",
    "            cmap = 'gray'\n",
    "        # display images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        axes[0].imshow(best_matched_image, cmap=cmap)\n",
    "        axes[0].set_title(f'{image_path.stem} Matches: {most_matches}')\n",
    "        axes[1].imshow(best_found_image, cmap=cmap)\n",
    "        axes[1].set_title(f'Title Guess: {best_title}')\n",
    "        plt.show()\n",
    "        \n",
    "        number_found_match += 1\n",
    "        \n",
    "    progress_bar.value = index\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
