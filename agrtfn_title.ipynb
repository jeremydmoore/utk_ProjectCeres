{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:85% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:01 PM\n"
     ]
    }
   ],
   "source": [
    "# imports & matplotlib options\n",
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import collections\n",
    "import random\n",
    "from shutil import copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from ipywidgets import interact, IntProgress, Label, VBox\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import img_qc.img_qc as img_qc\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from PIL import Image\n",
    "\n",
    "# plt.rc('figure', figsize=(30.0, 20.0))\n",
    "display(HTML(\"<style>.container {width:85% !important;}</style>\"))\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:02 PM\n"
     ]
    }
   ],
   "source": [
    "# pre-built lists to iterate over\n",
    "months = [\n",
    "    'january',\n",
    "    'february',\n",
    "    'march',\n",
    "    'april',\n",
    "    'may',\n",
    "    'june',\n",
    "    'july',\n",
    "    'august',\n",
    "    'september',\n",
    "    'october',\n",
    "    'november',\n",
    "    'december'\n",
    "]\n",
    "\n",
    "roi_box_list = ['text', 'left', 'top', 'width', 'height']\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3451 images in page 1 paths list\n",
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:05 PM\n"
     ]
    }
   ],
   "source": [
    "# create page 1 paths list\n",
    "data_dir_path = Path('/Volumes/jmoor167/data/agrtfn')\n",
    "page_1_paths_list = sorted(data_dir_path.glob('*.tif'))\n",
    "\n",
    "# delete macOS '.' index files\n",
    "regenerate_paths_list = False\n",
    "for path in page_1_paths_list:\n",
    "    if path.name.startswith('.'):\n",
    "        path.unlink()  # delete it\n",
    "        regenerate_paths_list = True\n",
    "\n",
    "if regenerate_paths_list:\n",
    "    page_1_paths_list = sorted(data_dir_path.glob('*.tif'))\n",
    "\n",
    "print(f'{len(page_1_paths_list)} images in page 1 paths list')\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:07 PM\n"
     ]
    }
   ],
   "source": [
    "# data for creating sub-crops to find\n",
    "\n",
    "# create OrderedDictionary for Title Crop data\n",
    "title_data_ordered_dict = OrderedDict()\n",
    "title_data_ordered_dict['Agricultural and home economics packet'] = OrderedDict(\n",
    "    [('rgb_0', [3295, (250, 450, 1450, 1150)])])\n",
    "title_data_ordered_dict['Agricultural news'] = OrderedDict(\n",
    "    [('rgb_0', [3446, (1150, 50, 3200, 850)])])\n",
    "title_data_ordered_dict['Farm news'] = OrderedDict(\n",
    "    [('gray_0', [2364, (1400, 500, 2650, 875)])])\n",
    "title_data_ordered_dict['Agricultural & home economics news'] = OrderedDict([\n",
    "    ('rgb_0', [2750, (1100, 50, 3150, 900)]),\n",
    "    ('rgb_1', [2951, (250, 500, 3050, 1100)])\n",
    "])\n",
    "title_data_ordered_dict['Tennessee farm and home news'] = OrderedDict([\n",
    "    ('gray_0', [1670, (1400, 150, 3150, 300)]),\n",
    "    ('gray_1', [964, (1400, 150, 3175, 290)]),\n",
    "    ('gray_2', [2151, (1250, 150, 3025, 325)])\n",
    "])\n",
    "title_data_ordered_dict['Tennessee farm news'] = OrderedDict([\n",
    "    ('gray_0', [5, (725, 525, 2100, 750)]),\n",
    "    ('gray_1', [15, (550, 600, 1950, 750)]),\n",
    "    ('gray_2', [456, (1000, 550, 2400, 675)]),\n",
    "    ('gray_3', [254, (800, 500, 2200, 650)]),\n",
    "    ('gray_4', [44, (850, 475, 2250, 600)])\n",
    "])\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     22,
     52,
     59,
     66,
     98,
     110
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:07 PM\n"
     ]
    }
   ],
   "source": [
    "# utility functions\n",
    "def crop_image_for_processing(image, percentage=0.4, top_and_sides_padding=10):\n",
    "\n",
    "    # get technical metadata\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # set (x, y) pairs\n",
    "    x1, y1, x2, y2 = 0, 0, width, int(height * percentage)\n",
    "\n",
    "    # add/subtract padding from the top/left/right\n",
    "    x1 += top_and_sides_padding\n",
    "    y1 += top_and_sides_padding\n",
    "    x2 -= top_and_sides_padding\n",
    "    # add image area to the bottom that was cropped from the top\n",
    "    y2 += top_and_sides_padding\n",
    "\n",
    "    # crop image\n",
    "    image = image[y1:y2, x1:x2]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_np_crop_points(crop_box):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    # print(f'crop box: {crop_box}')\n",
    "\n",
    "    # append all x/y points to their respective lists\n",
    "    for i in range(len(crop_box)):\n",
    "        if crop_box[i][0][0]:\n",
    "            x = (crop_box[i][0][0])\n",
    "            if x < 0:\n",
    "                x = -x\n",
    "            x_points.append(x)\n",
    "        if crop_box[i][0][1]:\n",
    "            y = crop_box[i][0][1]\n",
    "            if y < 0:\n",
    "                y = -y\n",
    "            y_points.append(y)\n",
    "    # print('x/y points')\n",
    "    # print(x_points, y_points)\n",
    "\n",
    "    # find extremes in crop box\n",
    "    x1 = min(x_points)\n",
    "    x2 = max(x_points)\n",
    "    y1 = min(y_points)\n",
    "    y2 = max(y_points)\n",
    "    # print(f'x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}')\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def if_rgb_convert_to_gray(np_image):\n",
    "    if len(np_image.shape) > 2:\n",
    "        np_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return np_image\n",
    "\n",
    "\n",
    "def if_bgr_convert_to_gray(np_image):\n",
    "    if len(np_image.shape) > 2:\n",
    "        np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return np_image\n",
    "\n",
    "\n",
    "def resize(image, width=None, height=None, ratio=None, inter=cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        ratio = height / float(h)\n",
    "        dim = (int(w * ratio), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        ratio = width / float(w)\n",
    "        dim = (width, int(h * ratio))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized, ratio\n",
    "\n",
    "\n",
    "def resize_ratio(image, ratio, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    dim = (int(w * ratio), int(h * ratio))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "\n",
    "def quick_imshow(bgr_image):\n",
    "    if len(bgr_image.shape) > 2:\n",
    "        bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "        cmap = None\n",
    "    else:\n",
    "        cmap = 'gray'\n",
    "    plt.imshow(bgr_image, cmap=cmap), plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:08 PM\n"
     ]
    }
   ],
   "source": [
    "# sub-crop finding classes\n",
    "class RootSIFT:\n",
    "    def __init__(self, extractor):\n",
    "        # initialize the SIFT feature extractor\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def compute(self, image, keypoints, epsilon=1e-7):\n",
    "        # compute SIFT descriptors\n",
    "        (keypoints, descriptors) = self.extractor.detectAndCompute(image, None)\n",
    "\n",
    "        # if there are are no keypoints or descriptors\n",
    "        if len(keypoints) == 0:\n",
    "            # return an empty tuple\n",
    "            return ([], None)\n",
    "\n",
    "        # apply the Hellinger kernel by first L1-normalizing and taking the\n",
    "        # square root\n",
    "        descriptors /= (descriptors.sum(axis=1, keepdims=True) + epsilon)\n",
    "        descriptors = np.sqrt(descriptors)\n",
    "\n",
    "        # return a tuple of the keypoints and descriptors\n",
    "        return (keypoints, descriptors)\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     3,
     68,
     84,
     183,
     209
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:08 PM\n"
     ]
    }
   ],
   "source": [
    "# image sub-crop finding functions\n",
    "\n",
    "\n",
    "def create_crop_dictionary(data_dictionary, show_images=False):\n",
    "\n",
    "    crop_dictionary = OrderedDict()\n",
    "    # bgr_crop_dictionary = OrderedDict()\n",
    "    # gray_crop_dictionary = OrderedDict()\n",
    "\n",
    "    for title, data in data_dictionary.items():\n",
    "\n",
    "        print(f'Processing {title} . . .')\n",
    "        print(f'\\t\\tsub_title\\tadminDB\\t\\tcrop_box (x1, y1, x2, y2)')\n",
    "        # print(data)\n",
    "\n",
    "        if title not in crop_dictionary:  # instantiate empty OrderedDict\n",
    "            crop_dictionary[title] = OrderedDict()\n",
    "            # print(crop_dictionary)\n",
    "\n",
    "        for sub_title, (adminDB, crop_box) in data.items():\n",
    "            print(f'\\t\\t{sub_title}\\t\\t{adminDB}\\t\\t{crop_box}')\n",
    "            # get image color, image_path, and crop box from sub_data\n",
    "            colorspace = sub_title.split('_')[0]\n",
    "\n",
    "            # print(colorspace)\n",
    "            adminDB_ending = f'{str(adminDB).zfill(6)}_0001.tif'\n",
    "            # print(adminDB_ending)\n",
    "            try:\n",
    "                image_path = [\n",
    "                    x for x in page_1_paths_list if x.name.endswith(adminDB_ending)][0]\n",
    "            except IndexError:  # no image match found\n",
    "                print(f'No image for {title} - {sub_title}\\n')\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = crop_box\n",
    "\n",
    "            # load image and crop it\n",
    "            image_for_title_crop = cv2.imread(str(image_path))\n",
    "            title_crop = image_for_title_crop[y1:y2, x1:x2].copy()\n",
    "\n",
    "            keypoints, descriptors = get_keypoints_and_descriptors(title_crop)\n",
    "\n",
    "            # add cropped image to dictionaries\n",
    "            # instantiate empty OrderedDict\n",
    "            if sub_title not in crop_dictionary[title]:\n",
    "                crop_dictionary[title][sub_title] = OrderedDict()\n",
    "                crop_dictionary[title][sub_title] = title_crop, keypoints, descriptors\n",
    "\n",
    "            # if colorspace == 'gray':  # then Grayscale\n",
    "            #     if title not in gray_crop_dictionary:\n",
    "            #         gray_crop_dictionary[title] = OrderedDict()\n",
    "            #     if sub_title not in gray_crop_dictionary[title]:\n",
    "            #         gray_crop_dictionary[title][sub_title] = title_crop\n",
    "            #     cmap = 'gray'\n",
    "            # else:  # then BGR\n",
    "            #     if title not in bgr_crop_dictionary:\n",
    "            #         bgr_crop_dictionary[title] = OrderedDict()\n",
    "            #     if sub_title not in bgr_crop_dictionary[title]:\n",
    "            #         bgr_crop_dictionary[title][sub_title] = title_crop\n",
    "            #     title_crop = cv2.cvtColor(title_crop, cv2.COLOR_BGR2RGB)\n",
    "            #     cmap = None\n",
    "\n",
    "            if show_images:\n",
    "                quick_imshow(title_crop)\n",
    "\n",
    "    return crop_dictionary  # , gray_crop_dictionary, bgr_crop_dictionary\n",
    "\n",
    "\n",
    "def get_keypoints_and_descriptors(image):\n",
    "    # convert to grayscale if necessary\n",
    "    image_gray = if_rgb_convert_to_gray(image)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    extractor = RootSIFT(sift)\n",
    "\n",
    "    # find keypoints and descriptors with RootSIFT\n",
    "    (keypoints, _) = sift.detectAndCompute(image_gray, None)\n",
    "    keypoints, descriptors = extractor.compute(image_gray, keypoints)\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html\n",
    "def find_crop_rootSift(image_search_for, image_look_in, minimum_matches, distance_ratio):\n",
    "\n",
    "    image_search_for = np.array(image_search_for)\n",
    "    image_look_in = np.array(image_look_in)\n",
    "\n",
    "    # convert to grayscale if necessary\n",
    "    image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "\n",
    "    # equalize histogram of image we're looking in (already done for title crop)\n",
    "    # image_look_in_gray = cv2.equalizeHist(image_look_in_gray)\n",
    "    # image_search_for_gray = cv2.equalizeHist(image_search_for_gray)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    extractor = RootSIFT(sift)\n",
    "\n",
    "    # find keypoints and descriptors with SIFT\n",
    "    (keypoints, _) = sift.detectAndCompute(image_search_for_gray, None)\n",
    "    keypoints_1, descriptors_1 = extractor.compute(\n",
    "        image_search_for_gray, keypoints)\n",
    "    (keypoints, _) = sift.detectAndCompute(image_look_in_gray, None)\n",
    "    keypoints_2, descriptors_2 = extractor.compute(\n",
    "        image_look_in_gray, keypoints)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_parameters = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "    # print(f'before: {number_of_good_matches}')\n",
    "\n",
    "    if number_of_good_matches >= minimum_matches:\n",
    "        source_points = np.float32(\n",
    "            [keypoints_1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        destination_points = np.float32(\n",
    "            [keypoints_2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        matrix, mask = cv2.findHomography(\n",
    "            source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "        if matrix is not None:\n",
    "\n",
    "            matches_mask = mask.ravel().tolist()\n",
    "\n",
    "            height, width = image_search_for_gray.shape\n",
    "            points = np.float32(\n",
    "                [[0, 0], [0, height-1], [width-1, height-1], [width-1, 0]]).reshape(-1, 1, 2)\n",
    "            destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "            crop_box = np.int32(destination)\n",
    "\n",
    "            match_drawing = image_look_in.copy()\n",
    "            match_drawing = cv2.polylines(match_drawing, [np.int32(\n",
    "                destination)], True, (255, 0, 0), 10, cv2.LINE_AA)\n",
    "\n",
    "            # draw matches\n",
    "            # draw_parameters = dict(\n",
    "            #     matchColor = (0, 255, 0),\n",
    "            #     singlePointColor = None,\n",
    "            #     matchesMask = matches_mask,  # only draw inliers\n",
    "            #     flags = 2  # don't draw single keypoints\n",
    "            # )\n",
    "\n",
    "            # matched_graphic = cv2.drawMatches(\n",
    "            #     image_search_for,\n",
    "            #     keypoints_1,\n",
    "            #     match_drawing,\n",
    "            #     keypoints_2,\n",
    "            #     good_matches,\n",
    "            #     None,\n",
    "            #     **draw_parameters\n",
    "            # )\n",
    "\n",
    "            # crop found image\n",
    "            perspective_matrix = cv2.getPerspectiveTransform(\n",
    "                np.float32(destination), points)\n",
    "            found_image = cv2.warpPerspective(\n",
    "                image_look_in, perspective_matrix, (width, height))\n",
    "\n",
    "            return number_of_good_matches, match_drawing, crop_box, found_image\n",
    "\n",
    "        # print('')\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print( \"Not enough matches are found - {}/{}\".format(len(good),minimum_matches))\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print('')\n",
    "\n",
    "    # return 0 for images if there weren't enough matches\n",
    "    return number_of_good_matches, None, None, None\n",
    "\n",
    "\n",
    "def get_descriptor_matches(descriptors_1, descriptors_2, distance_ratio):\n",
    "\n",
    "    # # load the image packages we're looking in and looking for, then convert to grayscale\n",
    "    # image_search_for, keypoints_1, descriptors_1 = to_find_image_package\n",
    "    # image_look_in, keypoints_2, descriptors_2 = find_in_image_package\n",
    "    # image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    # image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "\n",
    "    # match descriptors\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_parameters = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "\n",
    "    return number_of_good_matches, good_matches\n",
    "\n",
    "\n",
    "def get_matching_crop_and_box(matches, keypoints_to_find, keypoints_look_in, image_look_for, image_look_in):\n",
    "\n",
    "    crop_image, found_image, crop_box = False, False, False\n",
    "\n",
    "#     source_points = np.float32(\n",
    "#         [keypoints_to_find[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "#     destination_points = np.float32(\n",
    "#         [keypoints_look_in[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "#     matrix, mask = cv2.findHomography(\n",
    "#         source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "\n",
    "#     if matrix is not None:\n",
    "\n",
    "#         matches_mask = mask.ravel().tolist()\n",
    "\n",
    "#         height, width = image_look_in.shape[:2]\n",
    "#         print(f'h/w: {height}/{width}')\n",
    "\n",
    "    source_points = np.float32([keypoints_to_find[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    destination_points = np.float32([keypoints_look_in[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    matrix, mask = cv2.findHomography(\n",
    "        source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "    if matrix is not None:\n",
    "\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "\n",
    "        height, width = image_look_for.shape[:2]\n",
    "        # print(f'h/w: {height}/{width}')\n",
    "        points = np.float32(\n",
    "            [[0, 0], [0, height-1], [width-1, height-1], [width-1, 0]]).reshape(-1, 1, 2)\n",
    "        destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "        crop_box = np.int32(destination)\n",
    "\n",
    "        # crop found image\n",
    "        perspective_matrix = cv2.getPerspectiveTransform(\n",
    "            np.float32(destination), points)\n",
    "        found_image = cv2.warpPerspective(\n",
    "            image_look_in, perspective_matrix, (width, height))\n",
    "        # quick_imshow(found_image)\n",
    "\n",
    "    return found_image, crop_box\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Agricultural and home economics packet . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t3295\t\t(250, 450, 1450, 1150)\n",
      "Processing Agricultural news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t3446\t\t(1150, 50, 3200, 850)\n",
      "Processing Farm news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t2364\t\t(1400, 500, 2650, 875)\n",
      "Processing Agricultural & home economics news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t2750\t\t(1100, 50, 3150, 900)\n",
      "\t\trgb_1\t\t2951\t\t(250, 500, 3050, 1100)\n",
      "Processing Tennessee farm and home news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t1670\t\t(1400, 150, 3150, 300)\n",
      "\t\tgray_1\t\t964\t\t(1400, 150, 3175, 290)\n",
      "\t\tgray_2\t\t2151\t\t(1250, 150, 3025, 325)\n",
      "Processing Tennessee farm news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t5\t\t(725, 525, 2100, 750)\n",
      "\t\tgray_1\t\t15\t\t(550, 600, 1950, 750)\n",
      "\t\tgray_2\t\t456\t\t(1000, 550, 2400, 675)\n",
      "\t\tgray_3\t\t254\t\t(800, 500, 2200, 650)\n",
      "\t\tgray_4\t\t44\t\t(850, 475, 2250, 600)\n",
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:19 PM\n"
     ]
    }
   ],
   "source": [
    "# create crops to find\n",
    "title_crops_dict = create_crop_dictionary(\n",
    "    title_data_ordered_dict, show_images=False)\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_crops_dict is 39.18 megabytes\n"
     ]
    }
   ],
   "source": [
    "# recursive getsizeof Object and all items contained in it\n",
    "from sys import getsizeof\n",
    "\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "\n",
    "print(\n",
    "    f'title_crops_dict is {round(get_size(title_crops_dict)/1024/1024, 2)} megabytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-28 09:54:19 PM\n"
     ]
    }
   ],
   "source": [
    "class TnFarmNews:\n",
    "\n",
    "    def __init__(self, image_path, config=False):\n",
    "        \n",
    "        self.image_path = Path(image_path)\n",
    "\n",
    "        # set config dictionary\n",
    "        if config:\n",
    "            self.config = config\n",
    "        else:\n",
    "            self.config = {'crop_percentage': 0.4,\n",
    "                           'distance_ratio': 0.6,\n",
    "                           'minimum_matches': 9,\n",
    "                           'top_and_bottom_padding': 10,\n",
    "                           'resize_width': 1500}\n",
    "\n",
    "        # set variables\n",
    "        self.MOST_MATCHES = self.config['minimum_matches']\n",
    "        self.BEST_SSIM_WITH_TITLE = -1  # on a scale of -1->1, 1 is a perfect match\n",
    "        self.BEST_TITLE_CROP = False\n",
    "        self.BEST_TITLE = False\n",
    "        self.RESIZE_RATIO = False\n",
    "        \n",
    "    def preprocess_image(self, crop_percentage=None, top_and_bottom_padding=None):\n",
    "        \n",
    "        if not crop_percentage:\n",
    "            crop_percentage = self.config['crop_percentage']\n",
    "        if not top_and_bottom_padding:\n",
    "            top_and_bottom_padding = self.config['top_and_bottom_padding']\n",
    "\n",
    "        # load image\n",
    "        self.image = cv2.imread(str(self.image_path))\n",
    "\n",
    "        # get technical metadata\n",
    "        self.height, self.width = self.image.shape[:2]\n",
    "        if len(self.image.shape) > 2:\n",
    "            self.colorspace = 'gray'\n",
    "        else:\n",
    "            self.colorspace = 'rgb'\n",
    "\n",
    "        # crop image and get shape\n",
    "        self.image_cropped = crop_image_for_processing(\n",
    "            self.image, crop_percentage, top_and_bottom_padding)\n",
    "        self.height_cropped, self.width_cropped = self.image_cropped.shape[:2]\n",
    "        \n",
    "        return self.image_cropped\n",
    "    \n",
    "\n",
    "    def guess_title(self, crop_dictionary, image=None, debug=False):\n",
    "\n",
    "        if debug:\n",
    "            print(f'Guessing title of {self.image_path.name} . . .')\n",
    "\n",
    "        # resize cropped image and get features\n",
    "        # if self.RESIZE_RATIO:\n",
    "        #     image = resize_ratio(self.image_cropped, self.RESIZE_RATIO)\n",
    "        # else:\n",
    "        #     image, self.RESIZE_RATIO = resize(\n",
    "        #         self.image_cropped, width=self.config['resize_width'])\n",
    "        \n",
    "        if not image:\n",
    "            image = self.preprocess_image()\n",
    "        \n",
    "        self.keypoints, self.descriptors = get_keypoints_and_descriptors(image)\n",
    "\n",
    "        best_per_title_dict = OrderedDict()\n",
    "\n",
    "        for title, data in crop_dictionary.items():\n",
    "\n",
    "                if title not in best_per_title_dict:  # instantiate an OrderedDict()\n",
    "                    best_per_title_dict[title] = OrderedDict()  # number_of_matches, crop_box\n",
    "                    best_per_title_dict[title] = [0, False, False, False]\n",
    "                    if debug:\n",
    "                        print(title)\n",
    "\n",
    "                for sub_title, sub_data in crop_dictionary[title].items():\n",
    "\n",
    "                    crop_image, crop_keypoints, crop_descriptors = sub_data\n",
    "                    # quick_imshow(crop_image)\n",
    "\n",
    "                    number_of_matches, matches = get_descriptor_matches(\n",
    "                        crop_descriptors, self.descriptors, self.config['distance_ratio'])\n",
    "                    if debug:\n",
    "                        print(sub_title)\n",
    "                        print(f'# of matches/minMatches: {number_of_matches}/{(self.MOST_MATCHES * 0.5)}')\n",
    "\n",
    "                    if number_of_matches >= (self.MOST_MATCHES * 0.5):  # then it might be the best match\n",
    "\n",
    "                        found_image, crop_box = get_matching_crop_and_box(\n",
    "                                matches, crop_keypoints, self.keypoints, crop_image, image)\n",
    "\n",
    "                        try:\n",
    "                            if not crop_box:  # then we didn't get a valid crop_box\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\tFalse positive: invalid crop_box')\n",
    "                                continue\n",
    "                        except ValueError:  # valid numpy array throws a ValueError when testing existence\n",
    "\n",
    "                            if debug:\n",
    "                                print(f'crop box: {crop_box}')\n",
    "                            x1, y1, x2, y2 = get_np_crop_points(crop_box)\n",
    "\n",
    "                            # x1 = int(x1 / self.RESIZE_RATIO)\n",
    "                            # y1 = int(y1 / self.RESIZE_RATIO)\n",
    "                            # x2 = int(x2 / self.RESIZE_RATIO)\n",
    "                            # y2 = int(y2 / self.RESIZE_RATIO)\n",
    "\n",
    "                            # if width/height of title isn't over min_title_width/height pixels\n",
    "                            # it's not the title\n",
    "                            min_title_width = 900\n",
    "                            min_title_height = 50\n",
    "                            title_width = x2 - x1\n",
    "                            title_height = y2 - y1\n",
    "\n",
    "                            if (x2 - x1 < min_title_width):\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\t\\tFalse positive: width < minimum')\n",
    "                                    print(\n",
    "                                        f'\\t\\t\\t\\t{x2} - {x1} = {title_width} < {min_title_width}')\n",
    "                                continue\n",
    "                            elif (y2 - y1 < min_title_height):\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\t\\tFalse positive: height < minimum')\n",
    "                                    print(\n",
    "                                        f'\\t\\t\\t\\t{y2} - {y2} = {title_height} < {min_title_height}')\n",
    "                                continue\n",
    "                                \n",
    "                            if debug:\n",
    "                                print(x1, y1, x2, y2)\n",
    "                                crop = self.image_cropped[int(y1):int(y2), int(x1):int(x2)]\n",
    "                                print(f'cropped image')\n",
    "                                quick_imshow(crop)\n",
    "\n",
    "                            # convert to grayscale for ssim\n",
    "                            found_gray = if_bgr_convert_to_gray(found_image)\n",
    "                            crop_gray = if_bgr_convert_to_gray(crop_image)\n",
    "\n",
    "                            # get the structural similiarity index of the match with the image for a 2nd heuristic\n",
    "                            try:\n",
    "                                if debug:\n",
    "                                    print('found_gray')\n",
    "                                    quick_imshow(found_gray)\n",
    "                                    print(f'crop_gray')\n",
    "                                    quick_imshow(crop_gray)\n",
    "                                ssim_value = ssim(found_gray, crop_gray)\n",
    "                                if ssim_value > self.BEST_SSIM_WITH_TITLE:\n",
    "                                    if debug:\n",
    "                                        print(f'new best ssim: {title, ssim_value}')\n",
    "                                    self.BEST_SSIM_WITH_TITLE = ssim_value\n",
    "                                    points = [x1, y1, x2, y2]\n",
    "                                    self.BEST_TITLE_CROP = points\n",
    "                                    self.BEST_TITLE = title\n",
    "                                    self.MOST_MATCHES = number_of_matches\n",
    "                    \n",
    "                            except ValueError:  # image shape doesn't match, so most likely incorrect title\n",
    "                                if debug:\n",
    "                                    print('wrong dimensions')\n",
    "                                continue\n",
    "                    \n",
    "\n",
    "    def ocr(self):\n",
    "\n",
    "        image = cv2.cvtColor(self.image_cropped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # blur image\n",
    "        image = cv2.GaussianBlur(image, (3, 3), 1)\n",
    "\n",
    "        # binarize\n",
    "        self.binarized = cv2.adaptiveThreshold(\n",
    "            image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 175, 21)\n",
    "\n",
    "        # ocr\n",
    "        self.line_and_word_boxes = tool.image_to_string(\n",
    "            Image.fromarray(self.binarized),\n",
    "            lang='eng',\n",
    "            builder=pyocr.builders.LineBoxBuilder(),\n",
    "        )\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title_crop</th>\n",
       "      <th>title_guess</th>\n",
       "      <th>title_matches_number</th>\n",
       "      <th>title_ssim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[803, 563, 2200, 793]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>18</td>\n",
       "      <td>0.643979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[1972, 100, 6164, 767]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>6</td>\n",
       "      <td>0.316811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[452, 671, 3145, 831]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>10</td>\n",
       "      <td>0.486516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[540, 589, 1939, 739]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>1243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[593, 51, 2851, 2363]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>8</td>\n",
       "      <td>0.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[477, 301, 2006, 547]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>6</td>\n",
       "      <td>0.576308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[944, 45, 2049, 628]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>16</td>\n",
       "      <td>0.492864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[1991, 561, 6897, 6912]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>7</td>\n",
       "      <td>0.543094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[542, 541, 1755, 1578]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>13</td>\n",
       "      <td>0.493266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[42, 487, 2124, 1063]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>13</td>\n",
       "      <td>0.370564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[810, 411, 2226, 577]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>68</td>\n",
       "      <td>0.779180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[787, 423, 2240, 563]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>70</td>\n",
       "      <td>0.688052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[790, 402, 2279, 588]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>86</td>\n",
       "      <td>0.722076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[839, 464, 2239, 589]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>494</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>data/images/0012_004266_003302_0001.tif</td>\n",
       "      <td>[181, 452, 1383, 1153]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>304</td>\n",
       "      <td>0.664425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>data/images/0012_004266_003306_0001.tif</td>\n",
       "      <td>[164, 449, 1370, 1156]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>272</td>\n",
       "      <td>0.643696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>data/images/0012_004266_003309_0001.tif</td>\n",
       "      <td>[237, 464, 1439, 1164]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>311</td>\n",
       "      <td>0.664965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>data/images/0012_004266_003318_0001.tif</td>\n",
       "      <td>[162, 457, 1367, 1160]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>328</td>\n",
       "      <td>0.661722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>data/images/0012_004266_003320_0001.tif</td>\n",
       "      <td>[1175, 23, 3219, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>424</td>\n",
       "      <td>0.775449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>data/images/0012_004266_003331_0001.tif</td>\n",
       "      <td>[1155, 9, 3199, 816]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>444</td>\n",
       "      <td>0.791195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>data/images/0012_004266_003333_0001.tif</td>\n",
       "      <td>[1144, 6, 3194, 802]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>460</td>\n",
       "      <td>0.797569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>data/images/0012_004266_003340_0001.tif</td>\n",
       "      <td>[1164, 30, 3218, 830]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>418</td>\n",
       "      <td>0.792449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>data/images/0012_004266_003348_0001.tif</td>\n",
       "      <td>[1150, 3, 3206, 839]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>417</td>\n",
       "      <td>0.773038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>data/images/0012_004266_003349_0001.tif</td>\n",
       "      <td>[1141, 10, 3199, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>335</td>\n",
       "      <td>0.800305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>data/images/0012_004266_003357_0001.tif</td>\n",
       "      <td>[1153, 7, 3208, 821]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>463</td>\n",
       "      <td>0.803411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>data/images/0012_004266_003364_0001.tif</td>\n",
       "      <td>[1111, 16, 3167, 812]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>405</td>\n",
       "      <td>0.789419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>data/images/0012_004266_003365_0001.tif</td>\n",
       "      <td>[1072, 6, 3123, 822]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>417</td>\n",
       "      <td>0.803113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>data/images/0012_004266_003371_0001.tif</td>\n",
       "      <td>[1072, 16, 3118, 830]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>460</td>\n",
       "      <td>0.786809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>data/images/0012_004266_003372_0001.tif</td>\n",
       "      <td>[1082, 17, 3127, 831]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>384</td>\n",
       "      <td>0.793711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>data/images/0012_004266_003375_0001.tif</td>\n",
       "      <td>[1073, 36, 3119, 853]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>408</td>\n",
       "      <td>0.797859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>data/images/0012_004266_003377_0001.tif</td>\n",
       "      <td>[1093, 34, 3147, 848]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>448</td>\n",
       "      <td>0.795319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>data/images/0012_004266_003379_0001.tif</td>\n",
       "      <td>[1078, 13, 3135, 835]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>442</td>\n",
       "      <td>0.796339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>data/images/0012_004266_003397_0001.tif</td>\n",
       "      <td>[1098, 26, 3145, 844]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>407</td>\n",
       "      <td>0.793953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>data/images/0012_004266_003402_0001.tif</td>\n",
       "      <td>[1086, 15, 3139, 832]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>325</td>\n",
       "      <td>0.793640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>data/images/0012_004266_003403_0001.tif</td>\n",
       "      <td>[1091, 16, 3140, 828]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>328</td>\n",
       "      <td>0.792194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>data/images/0012_004266_003404_0001.tif</td>\n",
       "      <td>[1102, 19, 3147, 835]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>365</td>\n",
       "      <td>0.799084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>data/images/0012_004266_003409_0001.tif</td>\n",
       "      <td>[1106, 9, 3153, 826]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>442</td>\n",
       "      <td>0.801411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>data/images/0012_004266_003433_0001.tif</td>\n",
       "      <td>[1095, 9, 3150, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>420</td>\n",
       "      <td>0.793920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>data/images/0012_004266_003439_0001.tif</td>\n",
       "      <td>[1094, 4, 3147, 826]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>361</td>\n",
       "      <td>0.787304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>data/images/0012_004266_003440_0001.tif</td>\n",
       "      <td>[1098, 7, 3148, 823]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>427</td>\n",
       "      <td>0.789521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>data/images/0012_004266_003444_0001.tif</td>\n",
       "      <td>[1133, 29, 3177, 837]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>371</td>\n",
       "      <td>0.854731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>data/images/0012_004266_003449_0001.tif</td>\n",
       "      <td>[1542, 235, 3264, 390]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>140</td>\n",
       "      <td>0.668110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>data/images/0012_004266_003452_0001.tif</td>\n",
       "      <td>[944, 472, 2346, 605]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>153</td>\n",
       "      <td>0.796328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>data/images/0012_004266_003453_0001.tif</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "1     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "2     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "3     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "4     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "5     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "6     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "7     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "8     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "9     /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "10    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "11    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "12    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "13    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "14    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "15    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "16    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "17    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "18    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "19    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "20    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "21    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "22    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "23    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "24    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "25    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "26    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "27    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "28    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "29    /Volumes/jmoor167/data/tnfarmnews/0012_004266_...   \n",
       "...                                                 ...   \n",
       "2622            data/images/0012_004266_003302_0001.tif   \n",
       "2623            data/images/0012_004266_003306_0001.tif   \n",
       "2624            data/images/0012_004266_003309_0001.tif   \n",
       "2625            data/images/0012_004266_003318_0001.tif   \n",
       "2626            data/images/0012_004266_003320_0001.tif   \n",
       "2627            data/images/0012_004266_003331_0001.tif   \n",
       "2628            data/images/0012_004266_003333_0001.tif   \n",
       "2629            data/images/0012_004266_003340_0001.tif   \n",
       "2630            data/images/0012_004266_003348_0001.tif   \n",
       "2631            data/images/0012_004266_003349_0001.tif   \n",
       "2632            data/images/0012_004266_003357_0001.tif   \n",
       "2633            data/images/0012_004266_003364_0001.tif   \n",
       "2634            data/images/0012_004266_003365_0001.tif   \n",
       "2635            data/images/0012_004266_003371_0001.tif   \n",
       "2636            data/images/0012_004266_003372_0001.tif   \n",
       "2637            data/images/0012_004266_003375_0001.tif   \n",
       "2638            data/images/0012_004266_003377_0001.tif   \n",
       "2639            data/images/0012_004266_003379_0001.tif   \n",
       "2640            data/images/0012_004266_003397_0001.tif   \n",
       "2641            data/images/0012_004266_003402_0001.tif   \n",
       "2642            data/images/0012_004266_003403_0001.tif   \n",
       "2643            data/images/0012_004266_003404_0001.tif   \n",
       "2644            data/images/0012_004266_003409_0001.tif   \n",
       "2645            data/images/0012_004266_003433_0001.tif   \n",
       "2646            data/images/0012_004266_003439_0001.tif   \n",
       "2647            data/images/0012_004266_003440_0001.tif   \n",
       "2648            data/images/0012_004266_003444_0001.tif   \n",
       "2649            data/images/0012_004266_003449_0001.tif   \n",
       "2650            data/images/0012_004266_003452_0001.tif   \n",
       "2651            data/images/0012_004266_003453_0001.tif   \n",
       "\n",
       "                   title_crop                             title_guess  \\\n",
       "0                         NaN                                     NaN   \n",
       "1                         NaN                                     NaN   \n",
       "2                         NaN                                     NaN   \n",
       "3       [803, 563, 2200, 793]                     Tennessee farm news   \n",
       "4                         NaN                                     NaN   \n",
       "5      [1972, 100, 6164, 767]                     Tennessee farm news   \n",
       "6       [452, 671, 3145, 831]                     Tennessee farm news   \n",
       "7       [540, 589, 1939, 739]                     Tennessee farm news   \n",
       "8       [593, 51, 2851, 2363]                     Tennessee farm news   \n",
       "9       [477, 301, 2006, 547]                     Tennessee farm news   \n",
       "10                        NaN                                     NaN   \n",
       "11                        NaN                                     NaN   \n",
       "12                        NaN                                     NaN   \n",
       "13                        NaN                                     NaN   \n",
       "14                        NaN                                     NaN   \n",
       "15                        NaN                                     NaN   \n",
       "16                        NaN                                     NaN   \n",
       "17       [944, 45, 2049, 628]                     Tennessee farm news   \n",
       "18                        NaN                                     NaN   \n",
       "19    [1991, 561, 6897, 6912]                     Tennessee farm news   \n",
       "20                        NaN                                     NaN   \n",
       "21                        NaN                                     NaN   \n",
       "22     [542, 541, 1755, 1578]                     Tennessee farm news   \n",
       "23      [42, 487, 2124, 1063]                     Tennessee farm news   \n",
       "24                        NaN                                     NaN   \n",
       "25                        NaN                                     NaN   \n",
       "26      [810, 411, 2226, 577]                     Tennessee farm news   \n",
       "27      [787, 423, 2240, 563]                     Tennessee farm news   \n",
       "28      [790, 402, 2279, 588]                     Tennessee farm news   \n",
       "29      [839, 464, 2239, 589]                     Tennessee farm news   \n",
       "...                       ...                                     ...   \n",
       "2622   [181, 452, 1383, 1153]  Agricultural and home economics packet   \n",
       "2623   [164, 449, 1370, 1156]  Agricultural and home economics packet   \n",
       "2624   [237, 464, 1439, 1164]  Agricultural and home economics packet   \n",
       "2625   [162, 457, 1367, 1160]  Agricultural and home economics packet   \n",
       "2626    [1175, 23, 3219, 827]                       Agricultural news   \n",
       "2627     [1155, 9, 3199, 816]                       Agricultural news   \n",
       "2628     [1144, 6, 3194, 802]                       Agricultural news   \n",
       "2629    [1164, 30, 3218, 830]                       Agricultural news   \n",
       "2630     [1150, 3, 3206, 839]                       Agricultural news   \n",
       "2631    [1141, 10, 3199, 827]                       Agricultural news   \n",
       "2632     [1153, 7, 3208, 821]                       Agricultural news   \n",
       "2633    [1111, 16, 3167, 812]                       Agricultural news   \n",
       "2634     [1072, 6, 3123, 822]                       Agricultural news   \n",
       "2635    [1072, 16, 3118, 830]                       Agricultural news   \n",
       "2636    [1082, 17, 3127, 831]                       Agricultural news   \n",
       "2637    [1073, 36, 3119, 853]                       Agricultural news   \n",
       "2638    [1093, 34, 3147, 848]                       Agricultural news   \n",
       "2639    [1078, 13, 3135, 835]                       Agricultural news   \n",
       "2640    [1098, 26, 3145, 844]                       Agricultural news   \n",
       "2641    [1086, 15, 3139, 832]                       Agricultural news   \n",
       "2642    [1091, 16, 3140, 828]                       Agricultural news   \n",
       "2643    [1102, 19, 3147, 835]                       Agricultural news   \n",
       "2644     [1106, 9, 3153, 826]                       Agricultural news   \n",
       "2645     [1095, 9, 3150, 827]                       Agricultural news   \n",
       "2646     [1094, 4, 3147, 826]                       Agricultural news   \n",
       "2647     [1098, 7, 3148, 823]                       Agricultural news   \n",
       "2648    [1133, 29, 3177, 837]                       Agricultural news   \n",
       "2649   [1542, 235, 3264, 390]            Tennessee farm and home news   \n",
       "2650    [944, 472, 2346, 605]                     Tennessee farm news   \n",
       "2651                      NaN                                     NaN   \n",
       "\n",
       "      title_matches_number  title_ssim  \n",
       "0                        9   -1.000000  \n",
       "1                        9   -1.000000  \n",
       "2                        9   -1.000000  \n",
       "3                       18    0.643979  \n",
       "4                        9   -1.000000  \n",
       "5                        6    0.316811  \n",
       "6                       10    0.486516  \n",
       "7                     1243    1.000000  \n",
       "8                        8    0.531900  \n",
       "9                        6    0.576308  \n",
       "10                       9   -1.000000  \n",
       "11                       9   -1.000000  \n",
       "12                       9   -1.000000  \n",
       "13                       9   -1.000000  \n",
       "14                       9   -1.000000  \n",
       "15                       9   -1.000000  \n",
       "16                       9   -1.000000  \n",
       "17                      16    0.492864  \n",
       "18                       9   -1.000000  \n",
       "19                       7    0.543094  \n",
       "20                       9   -1.000000  \n",
       "21                       9   -1.000000  \n",
       "22                      13    0.493266  \n",
       "23                      13    0.370564  \n",
       "24                       9   -1.000000  \n",
       "25                       9   -1.000000  \n",
       "26                      68    0.779180  \n",
       "27                      70    0.688052  \n",
       "28                      86    0.722076  \n",
       "29                     494    0.999998  \n",
       "...                    ...         ...  \n",
       "2622                   304    0.664425  \n",
       "2623                   272    0.643696  \n",
       "2624                   311    0.664965  \n",
       "2625                   328    0.661722  \n",
       "2626                   424    0.775449  \n",
       "2627                   444    0.791195  \n",
       "2628                   460    0.797569  \n",
       "2629                   418    0.792449  \n",
       "2630                   417    0.773038  \n",
       "2631                   335    0.800305  \n",
       "2632                   463    0.803411  \n",
       "2633                   405    0.789419  \n",
       "2634                   417    0.803113  \n",
       "2635                   460    0.786809  \n",
       "2636                   384    0.793711  \n",
       "2637                   408    0.797859  \n",
       "2638                   448    0.795319  \n",
       "2639                   442    0.796339  \n",
       "2640                   407    0.793953  \n",
       "2641                   325    0.793640  \n",
       "2642                   328    0.792194  \n",
       "2643                   365    0.799084  \n",
       "2644                   442    0.801411  \n",
       "2645                   420    0.793920  \n",
       "2646                   361    0.787304  \n",
       "2647                   427    0.789521  \n",
       "2648                   371    0.854731  \n",
       "2649                   140    0.668110  \n",
       "2650                   153    0.796328  \n",
       "2651                     9   -1.000000  \n",
       "\n",
       "[2652 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = data_dir_path.joinpath('agrtfn_title.csv')\n",
    "if csv_path.is_file():\n",
    "    processed_df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    processed_df = pd.DataFrame()\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title_crop</th>\n",
       "      <th>title_guess</th>\n",
       "      <th>title_matches_number</th>\n",
       "      <th>title_ssim</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0012_004266_000001_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0012_004266_000002_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0012_004266_000006_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>[803, 563, 2200, 793]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>18</td>\n",
       "      <td>0.643979</td>\n",
       "      <td>0012_004266_000009_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Volumes/jmoor167/data/tnfarmnews/0012_004266_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0012_004266_000011_0001.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path             title_crop  \\\n",
       "0  /Volumes/jmoor167/data/tnfarmnews/0012_004266_...                    NaN   \n",
       "1  /Volumes/jmoor167/data/tnfarmnews/0012_004266_...                    NaN   \n",
       "2  /Volumes/jmoor167/data/tnfarmnews/0012_004266_...                    NaN   \n",
       "3  /Volumes/jmoor167/data/tnfarmnews/0012_004266_...  [803, 563, 2200, 793]   \n",
       "4  /Volumes/jmoor167/data/tnfarmnews/0012_004266_...                    NaN   \n",
       "\n",
       "           title_guess  title_matches_number  title_ssim  \\\n",
       "0                  NaN                     9   -1.000000   \n",
       "1                  NaN                     9   -1.000000   \n",
       "2                  NaN                     9   -1.000000   \n",
       "3  Tennessee farm news                    18    0.643979   \n",
       "4                  NaN                     9   -1.000000   \n",
       "\n",
       "                    image_name  \n",
       "0  0012_004266_000001_0001.tif  \n",
       "1  0012_004266_000002_0001.tif  \n",
       "2  0012_004266_000006_0001.tif  \n",
       "3  0012_004266_000009_0001.tif  \n",
       "4  0012_004266_000011_0001.tif  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are duplicate rows due to different data paths\n",
    "# add image_name as a DataFrame column so this can be parsed\n",
    "processed_df['image_name'] = processed_df.path.str.split('/').str[-1]\n",
    "processed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_df = processed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_crop</th>\n",
       "      <th>title_guess</th>\n",
       "      <th>title_matches_number</th>\n",
       "      <th>title_ssim</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0012_004266_000001_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0012_004266_000002_0001.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0012_004266_000006_0001.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title_crop title_guess  title_matches_number  title_ssim  \\\n",
       "0        NaN         NaN                     9        -1.0   \n",
       "1        NaN         NaN                     9        -1.0   \n",
       "2        NaN         NaN                     9        -1.0   \n",
       "\n",
       "                    image_name  \n",
       "0  0012_004266_000001_0001.tif  \n",
       "1  0012_004266_000002_0001.tif  \n",
       "2  0012_004266_000006_0001.tif  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.drop(['path'], axis=1, inplace=True)\n",
    "processed_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f59900d17e746f98b40877f4f190295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Images to process'), IntProgress(value=0, max=3451)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data from 40 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 60 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 80 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 100 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 120 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 140 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 160 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 180 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 200 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 220 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 240 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 260 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 280 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 300 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 320 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 340 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 360 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 380 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 400 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 420 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 440 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 460 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 480 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 500 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 520 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 540 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 560 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 580 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 600 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 620 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 640 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 660 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 680 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 700 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 720 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 740 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 760 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 780 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 800 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 820 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 840 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 860 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 880 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 900 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 920 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 940 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 960 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 980 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1000 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1020 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1040 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1060 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1080 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1100 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1120 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1140 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1160 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1180 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1200 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1220 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1240 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1260 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1280 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1300 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1320 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1340 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1360 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1380 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1400 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1420 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1440 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1460 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1480 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1500 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1520 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1540 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1560 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1580 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1600 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1620 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1640 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1660 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1680 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1700 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1720 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1740 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1760 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1780 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1800 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1820 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1840 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1860 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1880 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1900 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1920 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1940 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1960 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 1980 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2000 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2020 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2040 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2060 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2080 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2100 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2120 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data from 2140 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2160 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2180 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2200 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2220 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2240 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2260 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2280 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2300 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2320 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2340 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2360 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2380 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2400 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2420 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2440 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2460 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2480 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2500 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2520 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2540 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2560 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2580 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2600 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2620 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2640 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2660 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2680 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2700 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2720 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2740 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2760 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2780 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2800 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2820 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2840 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2860 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2880 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2900 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2920 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2940 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2960 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 2980 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3000 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3020 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3040 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3060 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3080 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3100 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3120 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3140 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3160 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3180 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3200 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3220 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3240 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3260 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3280 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3300 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3320 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3340 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3360 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3380 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3400 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3420 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3440 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n",
      "Saving data from 3451 images to /Volumes/jmoor167/data/agrtfn/agrtfn_title.csv\n"
     ]
    }
   ],
   "source": [
    "# batch process\n",
    "rows_list = []\n",
    "\n",
    "images_to_process_before_saving_csv = 20\n",
    "\n",
    "# uncomment to reset processed_df\n",
    "# processed_df = pd.DataFrame()\n",
    "\n",
    "# let's process the pages in random order\n",
    "paths_list = page_1_paths_list\n",
    "# paths_list = random.sample(page_1_paths_list, len(page_1_paths_list))\n",
    "\n",
    "number_of_paths = len(paths_list)\n",
    "\n",
    "# progress bar\n",
    "progress_label = Label('Images to process')\n",
    "progress_bar = IntProgress(min=0, max=number_of_paths)\n",
    "progress_widget = VBox([progress_label, progress_bar])\n",
    "display(progress_widget)\n",
    "\n",
    "\n",
    "for index, image_path in enumerate(paths_list, start=1):\n",
    "\n",
    "    # if the image_path is already in the dataframe skip it\n",
    "    if len(processed_df) > 0:\n",
    "        # test for the paths that end with our filename as the filename MUST be unique\n",
    "        # anyway as it's based on the preservation identifier\n",
    "        if (processed_df['image_name'].str.endswith(str(image_path.name))).any():\n",
    "            # if modulo of processed images is 0 or it's the last image save data to the CSV \n",
    "            if index % images_to_process_before_saving_csv == 0 or index == number_of_paths:\n",
    "                if len(rows_list) > 0:\n",
    "\n",
    "                    print(f'Saving data from {index} images to {csv_path}')\n",
    "\n",
    "                    # get dataframe from processed rows\n",
    "                    crop_df = pd.DataFrame(rows_list)\n",
    "\n",
    "                    # add dataframes together\n",
    "                    processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "                    # drop duplicates\n",
    "                    processed_df = processed_df.iloc[processed_df.astype(str).drop_duplicates().index]\n",
    "\n",
    "                    # sort on image_path and reset the index\n",
    "                    processed_df = processed_df.sort_values(by='image_name').reset_index(drop=True)\n",
    "                    processed_df.to_csv(csv_path, index=False)\n",
    "\n",
    "                    # reset rows_list\n",
    "                    rows_list = []\n",
    "\n",
    "            continue\n",
    "\n",
    "    label = f'Processing {image_path.name} . . . {index}/{number_of_paths}'\n",
    "    progress_label.value = label\n",
    "\n",
    "    issue = TnFarmNews(image_path)\n",
    "    # issue.ocr()\n",
    "    # draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    issue.guess_title(title_crops_dict, debug=False)\n",
    "    # print(f'\\t{issue.BEST_TITLE}')\n",
    "\n",
    "    # get input row in dictionary format\n",
    "    # key = column_name\n",
    "    results_dictionary = {'image_name': issue.image_path.name,\n",
    "                          'title_guess': issue.BEST_TITLE,\n",
    "                          'title_crop': issue.BEST_TITLE_CROP,\n",
    "                          'title_matches_number': issue.MOST_MATCHES,\n",
    "                          'title_ssim': issue.BEST_SSIM_WITH_TITLE\n",
    "                          }\n",
    "\n",
    "    for key in results_dictionary:\n",
    "        if results_dictionary[key]:\n",
    "            continue\n",
    "        else:\n",
    "            results_dictionary.update({key: None})\n",
    "\n",
    "    rows_list.append(results_dictionary)\n",
    "\n",
    "    progress_bar.value = index\n",
    "    \n",
    "    # if modulo of processed images is 0 or it's the last image save data to the CSV \n",
    "    if index % images_to_process_before_saving_csv == 0 or index == number_of_paths:\n",
    "        \n",
    "        print(f'Saving data from {index} images to {csv_path}')\n",
    "        \n",
    "        # get dataframe from processed rows\n",
    "        crop_df = pd.DataFrame(rows_list)\n",
    "        \n",
    "        # add dataframes together\n",
    "        processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "        # drop duplicates\n",
    "        processed_df = processed_df.iloc[processed_df.astype(str).drop_duplicates().index]\n",
    "\n",
    "        # sort on image_path and reset the index\n",
    "        processed_df = processed_df.sort_values(by='image_name').reset_index(drop=True)\n",
    "        processed_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # reset rows_list\n",
    "        rows_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# batch process\n",
    "rows_list = []\n",
    "\n",
    "images_to_process_before_saving_csv = 20\n",
    "\n",
    "# uncomment to reset processed_df\n",
    "# processed_df = pd.DataFrame()\n",
    "\n",
    "# let's process the pages in random order\n",
    "# paths_list = page_1_paths_list\n",
    "paths_list = random.sample(page_1_paths_list, len(page_1_paths_list))\n",
    "\n",
    "number_of_paths = len(paths_list)\n",
    "\n",
    "# progress bar\n",
    "progress_label = Label('Images to process')\n",
    "progress_bar = IntProgress(min=0, max=number_of_paths)\n",
    "progress_widget = VBox([progress_label, progress_bar])\n",
    "display(progress_widget)\n",
    "\n",
    "\n",
    "for index, image_path in enumerate(paths_list, start=1):\n",
    "\n",
    "    # if the image_path is already in the dataframe skip it\n",
    "    if len(processed_df) > 0:\n",
    "        # test for the paths that end with our filename as the filename MUST be unique\n",
    "        # anyway as it's based on the preservation identifier\n",
    "        if (processed_df['path'].str.endswith(str(image_path.name))).any():\n",
    "            continue\n",
    "\n",
    "    label = f'Processing {image_path.name} . . . {index}/{number_of_paths}'\n",
    "    progress_label.value = label\n",
    "\n",
    "    issue = TnFarmNews(image_path)\n",
    "    # issue.ocr()\n",
    "    # draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    issue.guess_title(title_crops_dict, debug=False)\n",
    "    # print(f'\\t{issue.BEST_TITLE}')\n",
    "\n",
    "    # get input row in dictionary format\n",
    "    # key = column_name\n",
    "    results_dictionary = {'path': str(issue.image_path),\n",
    "                          'title_guess': issue.BEST_TITLE,\n",
    "                          'title_crop': issue.BEST_TITLE_CROP,\n",
    "                          'title_matches_number': issue.MOST_MATCHES,\n",
    "                          'title_ssim': issue.BEST_SSIM_WITH_TITLE\n",
    "                          }\n",
    "\n",
    "    for key in results_dictionary:\n",
    "        if results_dictionary[key]:\n",
    "            continue\n",
    "        else:\n",
    "            results_dictionary.update({key: None})\n",
    "\n",
    "    rows_list.append(results_dictionary)\n",
    "\n",
    "    progress_bar.value = index\n",
    "    \n",
    "    # if modulo of processed images is 0 or it's the last image save data to the CSV \n",
    "    if index % images_to_process_before_saving_csv == 0 or index == number_of_paths:\n",
    "        \n",
    "        print(f'Saving data from {index} images to {csv_path}')\n",
    "        \n",
    "        # get dataframe from processed rows\n",
    "        crop_df = pd.DataFrame(rows_list)\n",
    "        \n",
    "        # add dataframes together\n",
    "        processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "        # drop duplicates\n",
    "        processed_df.drop_duplicates(subset=['path'], inplace=True)\n",
    "\n",
    "        # sort on image_path and reset the index\n",
    "        processed_df = processed_df.sort_values(by='path').reset_index(drop=True)\n",
    "        processed_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # reset rows_list\n",
    "        rows_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get dataframe from processed rows\n",
    "crop_df = pd.DataFrame(rows_list)\n",
    "crop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataframes together\n",
    "processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "# drop duplicates\n",
    "processed_df.drop_duplicates(subset=['path'], inplace=True)\n",
    "\n",
    "# sort on image_path and reset the index\n",
    "processed_df = processed_df.sort_values(by='path').reset_index(drop=True)\n",
    "processed_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# based on low match data in the cell below, I changed the minimum matches to 10 to see if I could correct\n",
    "# the false positive on data/images/0012_004266_000395_0001.tif\n",
    "# I ended up dropping the values that were under 8 matches in the processed_df and saving out to CSV\n",
    "print(f'before drop: {len(processed_df)}')\n",
    "no_low_matches_df = processed_df[processed_df['title_matches_number'] >= 10].reset_index(drop=True)\n",
    "print(f'after drop: {len(no_low_matches_df)}')\n",
    "\n",
    "# processed_df = no_low_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string = 'string'\n",
    "isinstance(string, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(processed_df)):\n",
    "    path = processed_df.iloc[i]['path']\n",
    "    title = processed_df.iloc[i]['title_guess']\n",
    "    matches = processed_df.iloc[i]['title_matches_number']\n",
    "    crop_box = processed_df.iloc[i]['title_crop']\n",
    "    print(matches, path)\n",
    "    print(title)\n",
    "    if isinstance(crop_box, str):\n",
    "        crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "        points = crop_box.split(', ')\n",
    "        # print(points)\n",
    "        points = [int(x) for x in points]\n",
    "    elif isinstance(crop_box, list):\n",
    "        # print(f'crop_box: {crop_box}')\n",
    "        points = crop_box\n",
    "    else:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = points\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_matches_df = processed_df[processed_df['title_matches_number'] < 10].reset_index(drop=True)\n",
    "for i in range(len(low_matches_df)):\n",
    "    path = low_matches_df.iloc[i]['path']\n",
    "    title = low_matches_df.iloc[i]['title_guess']\n",
    "    matches = low_matches_df.iloc[i]['title_matches_number']\n",
    "    crop_box = low_matches_df.iloc[i]['title_crop']\n",
    "    # print(low_matches_df.iloc[i]['title_crop'])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # print(low_matches_df.iloc[i])\n",
    "    print(title, matches)\n",
    "    print(path)\n",
    "    print(type(crop_box))\n",
    "    print(crop_box)\n",
    "    print('')\n",
    "    if isinstance(crop_box, str):\n",
    "        print('string')\n",
    "        crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "        points = crop_box.split(', ')\n",
    "        # print(points)\n",
    "        points = [int(x) for x in points]\n",
    "    elif isinstance(crop_box, list):\n",
    "        print(f'crop_box: {crop_box}')\n",
    "        points = crop_box\n",
    "    else:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = points\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_matches_df = test[test['title_matches_number'] < 10].reset_index(drop=True)\n",
    "for i in range(len(low_matches_df)):\n",
    "    path = low_matches_df.iloc[i]['path']\n",
    "    title = low_matches_df.iloc[i]['title_guess']\n",
    "    matches = low_matches_df.iloc[i]['title_matches_number']\n",
    "    # print(low_matches_df.iloc[i]['title_crop'])\n",
    "\n",
    "    crop_box = low_matches_df.iloc[i]['title_crop']\n",
    "    crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "    points = crop_box.split(', ')\n",
    "    # print(points)\n",
    "    points = [int(x) for x in points]\n",
    "    x1, y1, x2, y2 = points\n",
    "\n",
    "    # print(low_matches_df.iloc[i])\n",
    "    print(title, matches)\n",
    "    if matches < 8:\n",
    "        print(path)\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_ssim_df = test[test['title_ssim'] < 0.5].reset_index(drop=True)\n",
    "for i in range(len(low_ssim_df)):\n",
    "    path = low_ssim_df.iloc[i]['path']\n",
    "    # print(low_ssim_df.iloc[i]['title_crop'])\n",
    "    title = low_ssim_df.iloc[i]['title_guess']\n",
    "    matches = low_ssim_df.iloc[i]['title_matches_number']\n",
    "    ssim = low_ssim_df.iloc[i]['title_ssim']\n",
    "    \n",
    "    crop_box = low_ssim_df.iloc[i]['title_crop']\n",
    "    crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "    points = crop_box.split(', ')\n",
    "    # print(points)\n",
    "    points = [int(x) for x in points]\n",
    "    x1, y1, x2, y2 = points\n",
    "    \n",
    "    # print(low_ssim_df.iloc[i])\n",
    "    print(title, ssim)\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(title_csv_path)\n",
    "test[test['title_matches_number'] < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = crop_df['image_path'][0]\n",
    "image = cv2.imread(str(image_path))\n",
    "x1, y1, x2, y2 = crop_df['title_crop'][0]\n",
    "image_cropped = image[y1:y2, x1:x2]\n",
    "plt.imshow(image), plt.show()\n",
    "plt.imshow(image_cropped), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_images(file=page_1_paths_list):\n",
    "    # load image\n",
    "    issue = TnFarmNews(file)\n",
    "    issue.ocr()\n",
    "    # draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    # image = Image.open(file)\n",
    "    # temp_image_path = Path('_temp_image.jpg')\n",
    "    # image.save(temp_image_path)\n",
    "    # display(ipyImage(temp_image_path))\n",
    "    issue.guess_title(title_crops_dict, debug=True)\n",
    "    print(issue.best_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# this is my attempt to try and make a crop-value widget!\n",
    "\n",
    "page_1_paths_dictionary = {path.name: path for path in page_1_paths_list}\n",
    "initial_path = page_1_paths_list[0]\n",
    "initial_images_key = initial_path.name\n",
    "initial_image = cv2.imread(image)\n",
    "initial_x2 =\n",
    "\n",
    "# Create widgets\n",
    "images = widgets.Dropdown(\n",
    "    options=page_1_paths_dictionary, value=initial_path_key)\n",
    "crop_x1 = widgets.IntRangeSlider(value=(100, , min=0, max=initial_image.shape[1])\n",
    "crop_y1=widgets.IntSlider(value=0, min=0, max=initial_image.shape[0])\n",
    "crop_x2=widgets.IntSlider()\n",
    "\n",
    "# Updates the image options based on directory value\n",
    "def update_crop(*args):\n",
    "    crop_x1.max=os.listdir(directory.value)\n",
    "\n",
    "# Tie the image options to directory value\n",
    "directory.observe(update_crop, 'value')\n",
    "\n",
    "# Show the images\n",
    "def show_images(fdir, file):\n",
    "    display(Image(f'{fdir}/{file}'))\n",
    "\n",
    "_=interact(show_images, fdir=directory, file=images)\n",
    "def crop_image(file=page_1_paths_list, x1=(0, Image.open(file).size[0])):\n",
    "    # load image\n",
    "    issue=TnFarmNews(file)\n",
    "    issue.ocr()\n",
    "    draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    # image = Image.open(file)\n",
    "    # temp_image_path = Path('_temp_image.jpg')\n",
    "    # image.save(temp_image_path)\n",
    "    # display(ipyImage(temp_image_path))\n",
    "    issue.guess_title(debug=True)\n",
    "    print(issue.best_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
