{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:16 PM\n"
     ]
    }
   ],
   "source": [
    "# imports & matplotlib options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import collections\n",
    "import random\n",
    "from shutil import copy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from ipywidgets import interact, IntProgress, Label, VBox\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import img_qc.img_qc as img_qc\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rc('figure', figsize=(30.0, 20.0))\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:17 PM\n"
     ]
    }
   ],
   "source": [
    "# pre-built lists to iterate over\n",
    "months = [\n",
    "    'january',\n",
    "    'february',\n",
    "    'march',\n",
    "    'april',\n",
    "    'may',\n",
    "    'june',\n",
    "    'july',\n",
    "    'august',\n",
    "    'september',\n",
    "    'october',\n",
    "    'november',\n",
    "    'december'\n",
    "]\n",
    "\n",
    "roi_box_list = ['text', 'left', 'top', 'width', 'height']\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3451 images in page 1 paths list\n",
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:17 PM\n"
     ]
    }
   ],
   "source": [
    "# create page 1 paths list\n",
    "data_dir_path = Path('/Volumes/jmoor167/data/')\n",
    "page_1_paths_list = sorted(data_dir_path.glob('tnfarmnews/*.tif'))\n",
    "\n",
    "# delete macOS '.' index files\n",
    "regenerate_paths_list = False\n",
    "for path in page_1_paths_list:\n",
    "    if path.name.startswith('.'):\n",
    "        path.unlink()\n",
    "        regenerate_paths_list = True\n",
    "\n",
    "if regenerate_paths_list:\n",
    "    page_1_paths_list = sorted(data_dir_path.glob('images/*.tif'))\n",
    "\n",
    "print(f'{len(page_1_paths_list)} images in page 1 paths list')\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:19 PM\n"
     ]
    }
   ],
   "source": [
    "# data for creating sub-crops to find\n",
    "\n",
    "# create OrderedDictionary for Title Crop data\n",
    "title_data_ordered_dict = OrderedDict()\n",
    "title_data_ordered_dict['Agricultural and home economics packet'] = OrderedDict(\n",
    "    [('rgb_0', [3295, (250, 450, 1450, 1150)])])\n",
    "title_data_ordered_dict['Agricultural news'] = OrderedDict(\n",
    "    [('rgb_0', [3446, (1150, 50, 3200, 850)])])\n",
    "title_data_ordered_dict['Farm news'] = OrderedDict(\n",
    "    [('gray_0', [2364, (1400, 500, 2650, 875)])])\n",
    "title_data_ordered_dict['Agricultural & home economics news'] = OrderedDict([\n",
    "    ('rgb_0', [2750, (1100, 50, 3150, 900)]),\n",
    "    ('rgb_1', [2951, (250, 500, 3050, 1100)])\n",
    "])\n",
    "title_data_ordered_dict['Tennessee farm and home news'] = OrderedDict([\n",
    "    ('gray_0', [1670, (1400, 150, 3150, 300)]),\n",
    "    ('gray_1', [964, (1400, 150, 3175, 290)]),\n",
    "    ('gray_2', [2151, (1250, 150, 3025, 325)])\n",
    "])\n",
    "title_data_ordered_dict['Tennessee farm news'] = OrderedDict([\n",
    "    ('gray_0', [5, (725, 525, 2100, 750)]),\n",
    "    ('gray_1', [15, (550, 600, 1950, 750)]),\n",
    "    ('gray_2', [456, (1000, 550, 2400, 675)]),\n",
    "    ('gray_3', [254, (800, 500, 2200, 650)]),\n",
    "    ('gray_4', [44, (850, 475, 2250, 600)])\n",
    "])\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     22,
     52,
     59,
     66,
     98,
     110
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:20 PM\n"
     ]
    }
   ],
   "source": [
    "# utility functions\n",
    "def crop_image_for_processing(image, percentage=0.4, top_and_sides_padding=10):\n",
    "\n",
    "    # get technical metadata\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # set (x, y) pairs\n",
    "    x1, y1, x2, y2 = 0, 0, width, int(height * percentage)\n",
    "\n",
    "    # add/subtract padding from the top/left/right\n",
    "    x1 += top_and_sides_padding\n",
    "    y1 += top_and_sides_padding\n",
    "    x2 -= top_and_sides_padding\n",
    "    # add image area to the bottom that was cropped from the top\n",
    "    y2 += top_and_sides_padding\n",
    "\n",
    "    # crop image\n",
    "    image = image[y1:y2, x1:x2]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_np_crop_points(crop_box):\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    # print(f'crop box: {crop_box}')\n",
    "\n",
    "    # append all x/y points to their respective lists\n",
    "    for i in range(len(crop_box)):\n",
    "        if crop_box[i][0][0]:\n",
    "            x = (crop_box[i][0][0])\n",
    "            if x < 0:\n",
    "                x = -x\n",
    "            x_points.append(x)\n",
    "        if crop_box[i][0][1]:\n",
    "            y = crop_box[i][0][1]\n",
    "            if y < 0:\n",
    "                y = -y\n",
    "            y_points.append(y)\n",
    "    # print('x/y points')\n",
    "    # print(x_points, y_points)\n",
    "\n",
    "    # find extremes in crop box\n",
    "    x1 = min(x_points)\n",
    "    x2 = max(x_points)\n",
    "    y1 = min(y_points)\n",
    "    y2 = max(y_points)\n",
    "    # print(f'x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}')\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def if_rgb_convert_to_gray(np_image):\n",
    "    if len(np_image.shape) > 2:\n",
    "        np_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return np_image\n",
    "\n",
    "\n",
    "def if_bgr_convert_to_gray(np_image):\n",
    "    if len(np_image.shape) > 2:\n",
    "        np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return np_image\n",
    "\n",
    "\n",
    "def resize(image, width=None, height=None, ratio=None, inter=cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        ratio = height / float(h)\n",
    "        dim = (int(w * ratio), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        ratio = width / float(w)\n",
    "        dim = (width, int(h * ratio))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized, ratio\n",
    "\n",
    "\n",
    "def resize_ratio(image, ratio, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    dim = (int(w * ratio), int(h * ratio))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "\n",
    "def quick_imshow(bgr_image):\n",
    "    if len(bgr_image.shape) > 2:\n",
    "        bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "        cmap = None\n",
    "    else:\n",
    "        cmap = 'gray'\n",
    "    plt.imshow(bgr_image, cmap=cmap), plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:20 PM\n"
     ]
    }
   ],
   "source": [
    "# sub-crop finding classes\n",
    "class RootSIFT:\n",
    "    def __init__(self, extractor):\n",
    "        # initialize the SIFT feature extractor\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def compute(self, image, keypoints, epsilon=1e-7):\n",
    "        # compute SIFT descriptors\n",
    "        (keypoints, descriptors) = self.extractor.detectAndCompute(image, None)\n",
    "\n",
    "        # if there are are no keypoints or descriptors\n",
    "        if len(keypoints) == 0:\n",
    "            # return an empty tuple\n",
    "            return ([], None)\n",
    "\n",
    "        # apply the Hellinger kernel by first L1-normalizing and taking the\n",
    "        # square root\n",
    "        descriptors /= (descriptors.sum(axis=1, keepdims=True) + epsilon)\n",
    "        descriptors = np.sqrt(descriptors)\n",
    "\n",
    "        # return a tuple of the keypoints and descriptors\n",
    "        return (keypoints, descriptors)\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     3,
     68,
     84,
     183,
     209
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:21 PM\n"
     ]
    }
   ],
   "source": [
    "# image sub-crop finding functions\n",
    "\n",
    "\n",
    "def create_crop_dictionary(data_dictionary, show_images=False):\n",
    "\n",
    "    crop_dictionary = OrderedDict()\n",
    "    # bgr_crop_dictionary = OrderedDict()\n",
    "    # gray_crop_dictionary = OrderedDict()\n",
    "\n",
    "    for title, data in data_dictionary.items():\n",
    "\n",
    "        print(f'Processing {title} . . .')\n",
    "        print(f'\\t\\tsub_title\\tadminDB\\t\\tcrop_box (x1, y1, x2, y2)')\n",
    "        # print(data)\n",
    "\n",
    "        if title not in crop_dictionary:  # instantiate empty OrderedDict\n",
    "            crop_dictionary[title] = OrderedDict()\n",
    "            # print(crop_dictionary)\n",
    "\n",
    "        for sub_title, (adminDB, crop_box) in data.items():\n",
    "            print(f'\\t\\t{sub_title}\\t\\t{adminDB}\\t\\t{crop_box}')\n",
    "            # get image color, image_path, and crop box from sub_data\n",
    "            colorspace = sub_title.split('_')[0]\n",
    "\n",
    "            # print(colorspace)\n",
    "            adminDB_ending = f'{str(adminDB).zfill(6)}_0001.tif'\n",
    "            # print(adminDB_ending)\n",
    "            try:\n",
    "                image_path = [\n",
    "                    x for x in page_1_paths_list if x.name.endswith(adminDB_ending)][0]\n",
    "            except IndexError:  # no image match found\n",
    "                print(f'No image for {title} - {sub_title}\\n')\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = crop_box\n",
    "\n",
    "            # load image and crop it\n",
    "            image_for_title_crop = cv2.imread(str(image_path))\n",
    "            title_crop = image_for_title_crop[y1:y2, x1:x2].copy()\n",
    "\n",
    "            keypoints, descriptors = get_keypoints_and_descriptors(title_crop)\n",
    "\n",
    "            # add cropped image to dictionaries\n",
    "            # instantiate empty OrderedDict\n",
    "            if sub_title not in crop_dictionary[title]:\n",
    "                crop_dictionary[title][sub_title] = OrderedDict()\n",
    "                crop_dictionary[title][sub_title] = title_crop, keypoints, descriptors\n",
    "\n",
    "            # if colorspace == 'gray':  # then Grayscale\n",
    "            #     if title not in gray_crop_dictionary:\n",
    "            #         gray_crop_dictionary[title] = OrderedDict()\n",
    "            #     if sub_title not in gray_crop_dictionary[title]:\n",
    "            #         gray_crop_dictionary[title][sub_title] = title_crop\n",
    "            #     cmap = 'gray'\n",
    "            # else:  # then BGR\n",
    "            #     if title not in bgr_crop_dictionary:\n",
    "            #         bgr_crop_dictionary[title] = OrderedDict()\n",
    "            #     if sub_title not in bgr_crop_dictionary[title]:\n",
    "            #         bgr_crop_dictionary[title][sub_title] = title_crop\n",
    "            #     title_crop = cv2.cvtColor(title_crop, cv2.COLOR_BGR2RGB)\n",
    "            #     cmap = None\n",
    "\n",
    "            if show_images:\n",
    "                quick_imshow(title_crop)\n",
    "\n",
    "    return crop_dictionary  # , gray_crop_dictionary, bgr_crop_dictionary\n",
    "\n",
    "\n",
    "def get_keypoints_and_descriptors(image):\n",
    "    # convert to grayscale if necessary\n",
    "    image_gray = if_rgb_convert_to_gray(image)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    extractor = RootSIFT(sift)\n",
    "\n",
    "    # find keypoints and descriptors with RootSIFT\n",
    "    (keypoints, _) = sift.detectAndCompute(image_gray, None)\n",
    "    keypoints, descriptors = extractor.compute(image_gray, keypoints)\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html\n",
    "def find_crop_rootSift(image_search_for, image_look_in, minimum_matches, distance_ratio):\n",
    "\n",
    "    image_search_for = np.array(image_search_for)\n",
    "    image_look_in = np.array(image_look_in)\n",
    "\n",
    "    # convert to grayscale if necessary\n",
    "    image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "\n",
    "    # equalize histogram of image we're looking in (already done for title crop)\n",
    "    # image_look_in_gray = cv2.equalizeHist(image_look_in_gray)\n",
    "    # image_search_for_gray = cv2.equalizeHist(image_search_for_gray)\n",
    "\n",
    "    # create SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    extractor = RootSIFT(sift)\n",
    "\n",
    "    # find keypoints and descriptors with SIFT\n",
    "    (keypoints, _) = sift.detectAndCompute(image_search_for_gray, None)\n",
    "    keypoints_1, descriptors_1 = extractor.compute(\n",
    "        image_search_for_gray, keypoints)\n",
    "    (keypoints, _) = sift.detectAndCompute(image_look_in_gray, None)\n",
    "    keypoints_2, descriptors_2 = extractor.compute(\n",
    "        image_look_in_gray, keypoints)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_parameters = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "    # print(f'before: {number_of_good_matches}')\n",
    "\n",
    "    if number_of_good_matches >= minimum_matches:\n",
    "        source_points = np.float32(\n",
    "            [keypoints_1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        destination_points = np.float32(\n",
    "            [keypoints_2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        matrix, mask = cv2.findHomography(\n",
    "            source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "        if matrix is not None:\n",
    "\n",
    "            matches_mask = mask.ravel().tolist()\n",
    "\n",
    "            height, width = image_search_for_gray.shape\n",
    "            points = np.float32(\n",
    "                [[0, 0], [0, height-1], [width-1, height-1], [width-1, 0]]).reshape(-1, 1, 2)\n",
    "            destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "            crop_box = np.int32(destination)\n",
    "\n",
    "            match_drawing = image_look_in.copy()\n",
    "            match_drawing = cv2.polylines(match_drawing, [np.int32(\n",
    "                destination)], True, (255, 0, 0), 10, cv2.LINE_AA)\n",
    "\n",
    "            # draw matches\n",
    "            # draw_parameters = dict(\n",
    "            #     matchColor = (0, 255, 0),\n",
    "            #     singlePointColor = None,\n",
    "            #     matchesMask = matches_mask,  # only draw inliers\n",
    "            #     flags = 2  # don't draw single keypoints\n",
    "            # )\n",
    "\n",
    "            # matched_graphic = cv2.drawMatches(\n",
    "            #     image_search_for,\n",
    "            #     keypoints_1,\n",
    "            #     match_drawing,\n",
    "            #     keypoints_2,\n",
    "            #     good_matches,\n",
    "            #     None,\n",
    "            #     **draw_parameters\n",
    "            # )\n",
    "\n",
    "            # crop found image\n",
    "            perspective_matrix = cv2.getPerspectiveTransform(\n",
    "                np.float32(destination), points)\n",
    "            found_image = cv2.warpPerspective(\n",
    "                image_look_in, perspective_matrix, (width, height))\n",
    "\n",
    "            return number_of_good_matches, match_drawing, crop_box, found_image\n",
    "\n",
    "        # print('')\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print( \"Not enough matches are found - {}/{}\".format(len(good),minimum_matches))\n",
    "        # print(f'{25 * \"*\"}')\n",
    "        # print('')\n",
    "\n",
    "    # return 0 for images if there weren't enough matches\n",
    "    return number_of_good_matches, None, None, None\n",
    "\n",
    "\n",
    "def get_descriptor_matches(descriptors_1, descriptors_2, distance_ratio):\n",
    "\n",
    "    # # load the image packages we're looking in and looking for, then convert to grayscale\n",
    "    # image_search_for, keypoints_1, descriptors_1 = to_find_image_package\n",
    "    # image_look_in, keypoints_2, descriptors_2 = find_in_image_package\n",
    "    # image_search_for_gray = if_rgb_convert_to_gray(image_search_for)\n",
    "    # image_look_in_gray = if_rgb_convert_to_gray(image_look_in)\n",
    "\n",
    "    # match descriptors\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_parameters = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_parameters = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_parameters, search_parameters)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < distance_ratio * n.distance:\n",
    "            good_matches.append(m)\n",
    "    number_of_good_matches = len(good_matches)\n",
    "\n",
    "    return number_of_good_matches, good_matches\n",
    "\n",
    "\n",
    "def get_matching_crop_and_box(matches, keypoints_to_find, keypoints_look_in, image_look_for, image_look_in):\n",
    "\n",
    "    crop_image, found_image, crop_box = False, False, False\n",
    "\n",
    "#     source_points = np.float32(\n",
    "#         [keypoints_to_find[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "#     destination_points = np.float32(\n",
    "#         [keypoints_look_in[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "#     matrix, mask = cv2.findHomography(\n",
    "#         source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "\n",
    "#     if matrix is not None:\n",
    "\n",
    "#         matches_mask = mask.ravel().tolist()\n",
    "\n",
    "#         height, width = image_look_in.shape[:2]\n",
    "#         print(f'h/w: {height}/{width}')\n",
    "\n",
    "    source_points = np.float32([keypoints_to_find[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    destination_points = np.float32([keypoints_look_in[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    matrix, mask = cv2.findHomography(\n",
    "        source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "    if matrix is not None:\n",
    "\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "\n",
    "        height, width = image_look_for.shape[:2]\n",
    "        # print(f'h/w: {height}/{width}')\n",
    "        points = np.float32(\n",
    "            [[0, 0], [0, height-1], [width-1, height-1], [width-1, 0]]).reshape(-1, 1, 2)\n",
    "        destination = cv2.perspectiveTransform(points, matrix)\n",
    "\n",
    "        crop_box = np.int32(destination)\n",
    "\n",
    "        # crop found image\n",
    "        perspective_matrix = cv2.getPerspectiveTransform(\n",
    "            np.float32(destination), points)\n",
    "        found_image = cv2.warpPerspective(\n",
    "            image_look_in, perspective_matrix, (width, height))\n",
    "        # quick_imshow(found_image)\n",
    "\n",
    "    return found_image, crop_box\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Agricultural and home economics packet . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t3295\t\t(250, 450, 1450, 1150)\n",
      "Processing Agricultural news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t3446\t\t(1150, 50, 3200, 850)\n",
      "Processing Farm news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t2364\t\t(1400, 500, 2650, 875)\n",
      "Processing Agricultural & home economics news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\trgb_0\t\t2750\t\t(1100, 50, 3150, 900)\n",
      "\t\trgb_1\t\t2951\t\t(250, 500, 3050, 1100)\n",
      "Processing Tennessee farm and home news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t1670\t\t(1400, 150, 3150, 300)\n",
      "\t\tgray_1\t\t964\t\t(1400, 150, 3175, 290)\n",
      "\t\tgray_2\t\t2151\t\t(1250, 150, 3025, 325)\n",
      "Processing Tennessee farm news . . .\n",
      "\t\tsub_title\tadminDB\t\tcrop_box (x1, y1, x2, y2)\n",
      "\t\tgray_0\t\t5\t\t(725, 525, 2100, 750)\n",
      "\t\tgray_1\t\t15\t\t(550, 600, 1950, 750)\n",
      "\t\tgray_2\t\t456\t\t(1000, 550, 2400, 675)\n",
      "\t\tgray_3\t\t254\t\t(800, 500, 2200, 650)\n",
      "\t\tgray_4\t\t44\t\t(850, 475, 2250, 600)\n",
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:33 PM\n"
     ]
    }
   ],
   "source": [
    "# create crops to find\n",
    "title_crops_dict = create_crop_dictionary(\n",
    "    title_data_ordered_dict, show_images=False)\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_crops_dict is 39.18 megabytes\n"
     ]
    }
   ],
   "source": [
    "# recursive getsizeof Object and all items contained in it\n",
    "from sys import getsizeof\n",
    "\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "\n",
    "print(\n",
    "    f'title_crops_dict is {round(get_size(title_crops_dict)/1024/1024, 2)} megabytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\n",
      "Last run on 2019-08-25 07:37:33 PM\n"
     ]
    }
   ],
   "source": [
    "class TnFarmNews:\n",
    "\n",
    "    def __init__(self, image_path, config=False):\n",
    "        \n",
    "        self.image_path = Path(image_path)\n",
    "\n",
    "        # set config dictionary\n",
    "        if config:\n",
    "            self.config = config\n",
    "        else:\n",
    "            self.config = {'crop_percentage': 0.4,\n",
    "                           'distance_ratio': 0.6,\n",
    "                           'minimum_matches': 9,\n",
    "                           'top_and_bottom_padding': 10,\n",
    "                           'resize_width': 1500}\n",
    "\n",
    "        # set variables\n",
    "        self.MOST_MATCHES = self.config['minimum_matches']\n",
    "        self.BEST_SSIM_WITH_TITLE = -1  # on a scale of -1->1, 1 is a perfect match\n",
    "        self.BEST_TITLE_CROP = False\n",
    "        self.BEST_TITLE = False\n",
    "        self.RESIZE_RATIO = False\n",
    "        \n",
    "    def preprocess_image(self, crop_percentage=None, top_and_bottom_padding=None):\n",
    "        \n",
    "        if not crop_percentage:\n",
    "            crop_percentage = self.config['crop_percentage']\n",
    "        if not top_and_bottom_padding:\n",
    "            top_and_bottom_padding = self.config['top_and_bottom_padding']\n",
    "\n",
    "        # load image\n",
    "        self.image = cv2.imread(str(self.image_path))\n",
    "\n",
    "        # get technical metadata\n",
    "        self.height, self.width = self.image.shape[:2]\n",
    "        if len(self.image.shape) > 2:\n",
    "            self.colorspace = 'gray'\n",
    "        else:\n",
    "            self.colorspace = 'rgb'\n",
    "\n",
    "        # crop image and get shape\n",
    "        self.image_cropped = crop_image_for_processing(\n",
    "            self.image, crop_percentage, top_and_bottom_padding)\n",
    "        self.height_cropped, self.width_cropped = self.image_cropped.shape[:2]\n",
    "        \n",
    "        return self.image_cropped\n",
    "    \n",
    "\n",
    "    def guess_title(self, crop_dictionary, image=None, debug=False):\n",
    "\n",
    "        if debug:\n",
    "            print(f'Guessing title of {self.image_path.name} . . .')\n",
    "\n",
    "        # resize cropped image and get features\n",
    "        # if self.RESIZE_RATIO:\n",
    "        #     image = resize_ratio(self.image_cropped, self.RESIZE_RATIO)\n",
    "        # else:\n",
    "        #     image, self.RESIZE_RATIO = resize(\n",
    "        #         self.image_cropped, width=self.config['resize_width'])\n",
    "        \n",
    "        if not image:\n",
    "            image = self.preprocess_image()\n",
    "        \n",
    "        self.keypoints, self.descriptors = get_keypoints_and_descriptors(image)\n",
    "\n",
    "        best_per_title_dict = OrderedDict()\n",
    "\n",
    "        for title, data in crop_dictionary.items():\n",
    "\n",
    "                if title not in best_per_title_dict:  # instantiate an OrderedDict()\n",
    "                    best_per_title_dict[title] = OrderedDict()  # number_of_matches, crop_box\n",
    "                    best_per_title_dict[title] = [0, False, False, False]\n",
    "                    if debug:\n",
    "                        print(title)\n",
    "\n",
    "                for sub_title, sub_data in crop_dictionary[title].items():\n",
    "\n",
    "                    crop_image, crop_keypoints, crop_descriptors = sub_data\n",
    "                    # quick_imshow(crop_image)\n",
    "\n",
    "                    number_of_matches, matches = get_descriptor_matches(\n",
    "                        crop_descriptors, self.descriptors, self.config['distance_ratio'])\n",
    "                    if debug:\n",
    "                        print(sub_title)\n",
    "                        print(f'# of matches/minMatches: {number_of_matches}/{(self.MOST_MATCHES * 0.5)}')\n",
    "\n",
    "                    if number_of_matches >= (self.MOST_MATCHES * 0.5):  # then it might be the best match\n",
    "\n",
    "                        found_image, crop_box = get_matching_crop_and_box(\n",
    "                                matches, crop_keypoints, self.keypoints, crop_image, image)\n",
    "\n",
    "                        try:\n",
    "                            if not crop_box:  # then we didn't get a valid crop_box\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\tFalse positive: invalid crop_box')\n",
    "                                continue\n",
    "                        except ValueError:  # valid numpy array throws a ValueError when testing existence\n",
    "\n",
    "                            if debug:\n",
    "                                print(f'crop box: {crop_box}')\n",
    "                            x1, y1, x2, y2 = get_np_crop_points(crop_box)\n",
    "\n",
    "                            # x1 = int(x1 / self.RESIZE_RATIO)\n",
    "                            # y1 = int(y1 / self.RESIZE_RATIO)\n",
    "                            # x2 = int(x2 / self.RESIZE_RATIO)\n",
    "                            # y2 = int(y2 / self.RESIZE_RATIO)\n",
    "\n",
    "                            # if width/height of title isn't over min_title_width/height pixels\n",
    "                            # it's not the title\n",
    "                            min_title_width = 900\n",
    "                            min_title_height = 50\n",
    "                            title_width = x2 - x1\n",
    "                            title_height = y2 - y1\n",
    "\n",
    "                            if (x2 - x1 < min_title_width):\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\t\\tFalse positive: width < minimum')\n",
    "                                    print(\n",
    "                                        f'\\t\\t\\t\\t{x2} - {x1} = {title_width} < {min_title_width}')\n",
    "                                continue\n",
    "                            elif (y2 - y1 < min_title_height):\n",
    "                                if debug:\n",
    "                                    print(f'\\t\\t\\tFalse positive: height < minimum')\n",
    "                                    print(\n",
    "                                        f'\\t\\t\\t\\t{y2} - {y2} = {title_height} < {min_title_height}')\n",
    "                                continue\n",
    "                                \n",
    "                            if debug:\n",
    "                                print(x1, y1, x2, y2)\n",
    "                                crop = self.image_cropped[int(y1):int(y2), int(x1):int(x2)]\n",
    "                                print(f'cropped image')\n",
    "                                quick_imshow(crop)\n",
    "\n",
    "                            # convert to grayscale for ssim\n",
    "                            found_gray = if_bgr_convert_to_gray(found_image)\n",
    "                            crop_gray = if_bgr_convert_to_gray(crop_image)\n",
    "\n",
    "                            # get the structural similiarity index of the match with the image for a 2nd heuristic\n",
    "                            try:\n",
    "                                if debug:\n",
    "                                    print('found_gray')\n",
    "                                    quick_imshow(found_gray)\n",
    "                                    print(f'crop_gray')\n",
    "                                    quick_imshow(crop_gray)\n",
    "                                ssim_value = ssim(found_gray, crop_gray)\n",
    "                                if ssim_value > self.BEST_SSIM_WITH_TITLE:\n",
    "                                    if debug:\n",
    "                                        print(f'new best ssim: {title, ssim_value}')\n",
    "                                    self.BEST_SSIM_WITH_TITLE = ssim_value\n",
    "                                    points = [x1, y1, x2, y2]\n",
    "                                    self.BEST_TITLE_CROP = points\n",
    "                                    self.BEST_TITLE = title\n",
    "                                    self.MOST_MATCHES = number_of_matches\n",
    "                    \n",
    "                            except ValueError:  # image shape doesn't match, so most likely incorrect title\n",
    "                                if debug:\n",
    "                                    print('wrong dimensions')\n",
    "                                continue\n",
    "                    \n",
    "\n",
    "    def ocr(self):\n",
    "\n",
    "        image = cv2.cvtColor(self.image_cropped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # blur image\n",
    "        image = cv2.GaussianBlur(image, (3, 3), 1)\n",
    "\n",
    "        # binarize\n",
    "        self.binarized = cv2.adaptiveThreshold(\n",
    "            image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 175, 21)\n",
    "\n",
    "        # ocr\n",
    "        self.line_and_word_boxes = tool.image_to_string(\n",
    "            Image.fromarray(self.binarized),\n",
    "            lang='eng',\n",
    "            builder=pyocr.builders.LineBoxBuilder(),\n",
    "        )\n",
    "\n",
    "\n",
    "print(f'\\n{25 * \"*_*\"}\\nLast run on {datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`your_df == “some_value”` : This will create a boolean mask of your whole DataFrame. If you want to search for directly in a column, use `df[‘your_column_name’]`. If you want to search on multiple columns, do `df[ [‘column_1’, ‘column_n’] ]`. For indexes, use either `df.loc[]` or `df.iloc[]`.\n",
    "Having a boolean array, you shall now use the method `.any()` on your newly created DataFrame, such as : `masked_df.any()`. This will return if any on the whole DataFrame has matched. But you can also specify if you want to make a search by column (`.any(axis=1)`), or row (`.any(axis=0)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title_crop</th>\n",
       "      <th>title_guess</th>\n",
       "      <th>title_matches_number</th>\n",
       "      <th>title_ssim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/images/0012_004266_000004_0001.tif</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/images/0012_004266_000007_0001.tif</td>\n",
       "      <td>[474, 274, 3949, 742]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>22</td>\n",
       "      <td>0.527573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/images/0012_004266_000028_0001.tif</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/images/0012_004266_000050_0001.tif</td>\n",
       "      <td>[791, 445, 2206, 571]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>92</td>\n",
       "      <td>0.732835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/images/0012_004266_000054_0001.tif</td>\n",
       "      <td>[845, 490, 2244, 620]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>110</td>\n",
       "      <td>0.911826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/images/0012_004266_000055_0001.tif</td>\n",
       "      <td>[755, 429, 2188, 586]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>92</td>\n",
       "      <td>0.753528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/images/0012_004266_000060_0001.tif</td>\n",
       "      <td>[686, 424, 2178, 586]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>68</td>\n",
       "      <td>0.706116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/images/0012_004266_000072_0001.tif</td>\n",
       "      <td>[673, 342, 2178, 507]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>70</td>\n",
       "      <td>0.697981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/images/0012_004266_000081_0001.tif</td>\n",
       "      <td>[771, 442, 2235, 603]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>88</td>\n",
       "      <td>0.722951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/images/0012_004266_000090_0001.tif</td>\n",
       "      <td>[783, 449, 2195, 605]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>80</td>\n",
       "      <td>0.717407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/images/0012_004266_000096_0001.tif</td>\n",
       "      <td>[760, 421, 2237, 597]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>74</td>\n",
       "      <td>0.717610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/images/0012_004266_000098_0001.tif</td>\n",
       "      <td>[762, 448, 2169, 599]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>95</td>\n",
       "      <td>0.736911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/images/0012_004266_000101_0001.tif</td>\n",
       "      <td>[733, 431, 2159, 586]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>102</td>\n",
       "      <td>0.737650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/images/0012_004266_000122_0001.tif</td>\n",
       "      <td>[749, 431, 2176, 586]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>72</td>\n",
       "      <td>0.727843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/images/0012_004266_000132_0001.tif</td>\n",
       "      <td>[755, 432, 2221, 591]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>99</td>\n",
       "      <td>0.722326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/images/0012_004266_000133_0001.tif</td>\n",
       "      <td>[763, 438, 2174, 590]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>66</td>\n",
       "      <td>0.768379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/images/0012_004266_000138_0001.tif</td>\n",
       "      <td>[736, 424, 2190, 581]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>74</td>\n",
       "      <td>0.716187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/images/0012_004266_000147_0001.tif</td>\n",
       "      <td>[793, 443, 2278, 605]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>77</td>\n",
       "      <td>0.711782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/images/0012_004266_000178_0001.tif</td>\n",
       "      <td>[797, 366, 2234, 500]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>79</td>\n",
       "      <td>0.703912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/images/0012_004266_000181_0001.tif</td>\n",
       "      <td>[800, 401, 2199, 532]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>118</td>\n",
       "      <td>0.911224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/images/0012_004266_000183_0001.tif</td>\n",
       "      <td>[833, 391, 2231, 518]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>115</td>\n",
       "      <td>0.914065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/images/0012_004266_000194_0001.tif</td>\n",
       "      <td>[787, 371, 2194, 506]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>69</td>\n",
       "      <td>0.752399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>data/images/0012_004266_000211_0001.tif</td>\n",
       "      <td>[758, 483, 2180, 635]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>95</td>\n",
       "      <td>0.749683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>data/images/0012_004266_000247_0001.tif</td>\n",
       "      <td>[759, 460, 2201, 620]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>91</td>\n",
       "      <td>0.753366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>data/images/0012_004266_000251_0001.tif</td>\n",
       "      <td>[745, 477, 2136, 630]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>89</td>\n",
       "      <td>0.732997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>data/images/0012_004266_000257_0001.tif</td>\n",
       "      <td>[778, 481, 2271, 638]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>75</td>\n",
       "      <td>0.720250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>data/images/0012_004266_000262_0001.tif</td>\n",
       "      <td>[728, 515, 2338, 675]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>13</td>\n",
       "      <td>0.516503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>data/images/0012_004266_000263_0001.tif</td>\n",
       "      <td>[705, 528, 2247, 658]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>8</td>\n",
       "      <td>0.569589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>data/images/0012_004266_000265_0001.tif</td>\n",
       "      <td>[725, 560, 2409, 702]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>11</td>\n",
       "      <td>0.464895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>data/images/0012_004266_000275_0001.tif</td>\n",
       "      <td>[845, 544, 2849, 700]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>11</td>\n",
       "      <td>0.431412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>data/images/0012_004266_003302_0001.tif</td>\n",
       "      <td>[181, 452, 1383, 1153]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>304</td>\n",
       "      <td>0.664425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>data/images/0012_004266_003306_0001.tif</td>\n",
       "      <td>[164, 449, 1370, 1156]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>272</td>\n",
       "      <td>0.643696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>data/images/0012_004266_003309_0001.tif</td>\n",
       "      <td>[237, 464, 1439, 1164]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>311</td>\n",
       "      <td>0.664965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>data/images/0012_004266_003318_0001.tif</td>\n",
       "      <td>[162, 457, 1367, 1160]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>328</td>\n",
       "      <td>0.661722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>data/images/0012_004266_003320_0001.tif</td>\n",
       "      <td>[1175, 23, 3219, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>424</td>\n",
       "      <td>0.775449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>data/images/0012_004266_003331_0001.tif</td>\n",
       "      <td>[1155, 9, 3199, 816]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>444</td>\n",
       "      <td>0.791195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>data/images/0012_004266_003333_0001.tif</td>\n",
       "      <td>[1144, 6, 3194, 802]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>460</td>\n",
       "      <td>0.797569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>data/images/0012_004266_003340_0001.tif</td>\n",
       "      <td>[1164, 30, 3218, 830]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>418</td>\n",
       "      <td>0.792449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>data/images/0012_004266_003348_0001.tif</td>\n",
       "      <td>[1150, 3, 3206, 839]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>417</td>\n",
       "      <td>0.773038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>data/images/0012_004266_003349_0001.tif</td>\n",
       "      <td>[1141, 10, 3199, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>335</td>\n",
       "      <td>0.800305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>data/images/0012_004266_003357_0001.tif</td>\n",
       "      <td>[1153, 7, 3208, 821]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>463</td>\n",
       "      <td>0.803411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>data/images/0012_004266_003364_0001.tif</td>\n",
       "      <td>[1111, 16, 3167, 812]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>405</td>\n",
       "      <td>0.789419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>data/images/0012_004266_003365_0001.tif</td>\n",
       "      <td>[1072, 6, 3123, 822]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>417</td>\n",
       "      <td>0.803113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>data/images/0012_004266_003371_0001.tif</td>\n",
       "      <td>[1072, 16, 3118, 830]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>460</td>\n",
       "      <td>0.786809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>data/images/0012_004266_003372_0001.tif</td>\n",
       "      <td>[1082, 17, 3127, 831]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>384</td>\n",
       "      <td>0.793711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>data/images/0012_004266_003375_0001.tif</td>\n",
       "      <td>[1073, 36, 3119, 853]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>408</td>\n",
       "      <td>0.797859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>data/images/0012_004266_003377_0001.tif</td>\n",
       "      <td>[1093, 34, 3147, 848]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>448</td>\n",
       "      <td>0.795319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>data/images/0012_004266_003379_0001.tif</td>\n",
       "      <td>[1078, 13, 3135, 835]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>442</td>\n",
       "      <td>0.796339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>data/images/0012_004266_003397_0001.tif</td>\n",
       "      <td>[1098, 26, 3145, 844]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>407</td>\n",
       "      <td>0.793953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>data/images/0012_004266_003402_0001.tif</td>\n",
       "      <td>[1086, 15, 3139, 832]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>325</td>\n",
       "      <td>0.793640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>data/images/0012_004266_003403_0001.tif</td>\n",
       "      <td>[1091, 16, 3140, 828]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>328</td>\n",
       "      <td>0.792194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>data/images/0012_004266_003404_0001.tif</td>\n",
       "      <td>[1102, 19, 3147, 835]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>365</td>\n",
       "      <td>0.799084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>data/images/0012_004266_003409_0001.tif</td>\n",
       "      <td>[1106, 9, 3153, 826]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>442</td>\n",
       "      <td>0.801411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>data/images/0012_004266_003433_0001.tif</td>\n",
       "      <td>[1095, 9, 3150, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>420</td>\n",
       "      <td>0.793920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>data/images/0012_004266_003439_0001.tif</td>\n",
       "      <td>[1094, 4, 3147, 826]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>361</td>\n",
       "      <td>0.787304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>data/images/0012_004266_003440_0001.tif</td>\n",
       "      <td>[1098, 7, 3148, 823]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>427</td>\n",
       "      <td>0.789521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>data/images/0012_004266_003444_0001.tif</td>\n",
       "      <td>[1133, 29, 3177, 837]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>371</td>\n",
       "      <td>0.854731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>data/images/0012_004266_003449_0001.tif</td>\n",
       "      <td>[1542, 235, 3264, 390]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>140</td>\n",
       "      <td>0.668110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>data/images/0012_004266_003452_0001.tif</td>\n",
       "      <td>[944, 472, 2346, 605]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>153</td>\n",
       "      <td>0.796328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>data/images/0012_004266_003453_0001.tif</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path              title_crop  \\\n",
       "0    data/images/0012_004266_000004_0001.tif                     NaN   \n",
       "1    data/images/0012_004266_000007_0001.tif   [474, 274, 3949, 742]   \n",
       "2    data/images/0012_004266_000028_0001.tif                     NaN   \n",
       "3    data/images/0012_004266_000050_0001.tif   [791, 445, 2206, 571]   \n",
       "4    data/images/0012_004266_000054_0001.tif   [845, 490, 2244, 620]   \n",
       "5    data/images/0012_004266_000055_0001.tif   [755, 429, 2188, 586]   \n",
       "6    data/images/0012_004266_000060_0001.tif   [686, 424, 2178, 586]   \n",
       "7    data/images/0012_004266_000072_0001.tif   [673, 342, 2178, 507]   \n",
       "8    data/images/0012_004266_000081_0001.tif   [771, 442, 2235, 603]   \n",
       "9    data/images/0012_004266_000090_0001.tif   [783, 449, 2195, 605]   \n",
       "10   data/images/0012_004266_000096_0001.tif   [760, 421, 2237, 597]   \n",
       "11   data/images/0012_004266_000098_0001.tif   [762, 448, 2169, 599]   \n",
       "12   data/images/0012_004266_000101_0001.tif   [733, 431, 2159, 586]   \n",
       "13   data/images/0012_004266_000122_0001.tif   [749, 431, 2176, 586]   \n",
       "14   data/images/0012_004266_000132_0001.tif   [755, 432, 2221, 591]   \n",
       "15   data/images/0012_004266_000133_0001.tif   [763, 438, 2174, 590]   \n",
       "16   data/images/0012_004266_000138_0001.tif   [736, 424, 2190, 581]   \n",
       "17   data/images/0012_004266_000147_0001.tif   [793, 443, 2278, 605]   \n",
       "18   data/images/0012_004266_000178_0001.tif   [797, 366, 2234, 500]   \n",
       "19   data/images/0012_004266_000181_0001.tif   [800, 401, 2199, 532]   \n",
       "20   data/images/0012_004266_000183_0001.tif   [833, 391, 2231, 518]   \n",
       "21   data/images/0012_004266_000194_0001.tif   [787, 371, 2194, 506]   \n",
       "22   data/images/0012_004266_000211_0001.tif   [758, 483, 2180, 635]   \n",
       "23   data/images/0012_004266_000247_0001.tif   [759, 460, 2201, 620]   \n",
       "24   data/images/0012_004266_000251_0001.tif   [745, 477, 2136, 630]   \n",
       "25   data/images/0012_004266_000257_0001.tif   [778, 481, 2271, 638]   \n",
       "26   data/images/0012_004266_000262_0001.tif   [728, 515, 2338, 675]   \n",
       "27   data/images/0012_004266_000263_0001.tif   [705, 528, 2247, 658]   \n",
       "28   data/images/0012_004266_000265_0001.tif   [725, 560, 2409, 702]   \n",
       "29   data/images/0012_004266_000275_0001.tif   [845, 544, 2849, 700]   \n",
       "..                                       ...                     ...   \n",
       "421  data/images/0012_004266_003302_0001.tif  [181, 452, 1383, 1153]   \n",
       "422  data/images/0012_004266_003306_0001.tif  [164, 449, 1370, 1156]   \n",
       "423  data/images/0012_004266_003309_0001.tif  [237, 464, 1439, 1164]   \n",
       "424  data/images/0012_004266_003318_0001.tif  [162, 457, 1367, 1160]   \n",
       "425  data/images/0012_004266_003320_0001.tif   [1175, 23, 3219, 827]   \n",
       "426  data/images/0012_004266_003331_0001.tif    [1155, 9, 3199, 816]   \n",
       "427  data/images/0012_004266_003333_0001.tif    [1144, 6, 3194, 802]   \n",
       "428  data/images/0012_004266_003340_0001.tif   [1164, 30, 3218, 830]   \n",
       "429  data/images/0012_004266_003348_0001.tif    [1150, 3, 3206, 839]   \n",
       "430  data/images/0012_004266_003349_0001.tif   [1141, 10, 3199, 827]   \n",
       "431  data/images/0012_004266_003357_0001.tif    [1153, 7, 3208, 821]   \n",
       "432  data/images/0012_004266_003364_0001.tif   [1111, 16, 3167, 812]   \n",
       "433  data/images/0012_004266_003365_0001.tif    [1072, 6, 3123, 822]   \n",
       "434  data/images/0012_004266_003371_0001.tif   [1072, 16, 3118, 830]   \n",
       "435  data/images/0012_004266_003372_0001.tif   [1082, 17, 3127, 831]   \n",
       "436  data/images/0012_004266_003375_0001.tif   [1073, 36, 3119, 853]   \n",
       "437  data/images/0012_004266_003377_0001.tif   [1093, 34, 3147, 848]   \n",
       "438  data/images/0012_004266_003379_0001.tif   [1078, 13, 3135, 835]   \n",
       "439  data/images/0012_004266_003397_0001.tif   [1098, 26, 3145, 844]   \n",
       "440  data/images/0012_004266_003402_0001.tif   [1086, 15, 3139, 832]   \n",
       "441  data/images/0012_004266_003403_0001.tif   [1091, 16, 3140, 828]   \n",
       "442  data/images/0012_004266_003404_0001.tif   [1102, 19, 3147, 835]   \n",
       "443  data/images/0012_004266_003409_0001.tif    [1106, 9, 3153, 826]   \n",
       "444  data/images/0012_004266_003433_0001.tif    [1095, 9, 3150, 827]   \n",
       "445  data/images/0012_004266_003439_0001.tif    [1094, 4, 3147, 826]   \n",
       "446  data/images/0012_004266_003440_0001.tif    [1098, 7, 3148, 823]   \n",
       "447  data/images/0012_004266_003444_0001.tif   [1133, 29, 3177, 837]   \n",
       "448  data/images/0012_004266_003449_0001.tif  [1542, 235, 3264, 390]   \n",
       "449  data/images/0012_004266_003452_0001.tif   [944, 472, 2346, 605]   \n",
       "450  data/images/0012_004266_003453_0001.tif                     NaN   \n",
       "\n",
       "                                title_guess  title_matches_number  title_ssim  \n",
       "0                                       NaN                     9   -1.000000  \n",
       "1                       Tennessee farm news                    22    0.527573  \n",
       "2                                       NaN                     9   -1.000000  \n",
       "3                       Tennessee farm news                    92    0.732835  \n",
       "4                       Tennessee farm news                   110    0.911826  \n",
       "5                       Tennessee farm news                    92    0.753528  \n",
       "6                       Tennessee farm news                    68    0.706116  \n",
       "7                       Tennessee farm news                    70    0.697981  \n",
       "8                       Tennessee farm news                    88    0.722951  \n",
       "9                       Tennessee farm news                    80    0.717407  \n",
       "10                      Tennessee farm news                    74    0.717610  \n",
       "11                      Tennessee farm news                    95    0.736911  \n",
       "12                      Tennessee farm news                   102    0.737650  \n",
       "13                      Tennessee farm news                    72    0.727843  \n",
       "14                      Tennessee farm news                    99    0.722326  \n",
       "15                      Tennessee farm news                    66    0.768379  \n",
       "16                      Tennessee farm news                    74    0.716187  \n",
       "17                      Tennessee farm news                    77    0.711782  \n",
       "18                      Tennessee farm news                    79    0.703912  \n",
       "19                      Tennessee farm news                   118    0.911224  \n",
       "20                      Tennessee farm news                   115    0.914065  \n",
       "21                      Tennessee farm news                    69    0.752399  \n",
       "22                      Tennessee farm news                    95    0.749683  \n",
       "23                      Tennessee farm news                    91    0.753366  \n",
       "24                      Tennessee farm news                    89    0.732997  \n",
       "25                      Tennessee farm news                    75    0.720250  \n",
       "26                      Tennessee farm news                    13    0.516503  \n",
       "27                      Tennessee farm news                     8    0.569589  \n",
       "28                      Tennessee farm news                    11    0.464895  \n",
       "29                      Tennessee farm news                    11    0.431412  \n",
       "..                                      ...                   ...         ...  \n",
       "421  Agricultural and home economics packet                   304    0.664425  \n",
       "422  Agricultural and home economics packet                   272    0.643696  \n",
       "423  Agricultural and home economics packet                   311    0.664965  \n",
       "424  Agricultural and home economics packet                   328    0.661722  \n",
       "425                       Agricultural news                   424    0.775449  \n",
       "426                       Agricultural news                   444    0.791195  \n",
       "427                       Agricultural news                   460    0.797569  \n",
       "428                       Agricultural news                   418    0.792449  \n",
       "429                       Agricultural news                   417    0.773038  \n",
       "430                       Agricultural news                   335    0.800305  \n",
       "431                       Agricultural news                   463    0.803411  \n",
       "432                       Agricultural news                   405    0.789419  \n",
       "433                       Agricultural news                   417    0.803113  \n",
       "434                       Agricultural news                   460    0.786809  \n",
       "435                       Agricultural news                   384    0.793711  \n",
       "436                       Agricultural news                   408    0.797859  \n",
       "437                       Agricultural news                   448    0.795319  \n",
       "438                       Agricultural news                   442    0.796339  \n",
       "439                       Agricultural news                   407    0.793953  \n",
       "440                       Agricultural news                   325    0.793640  \n",
       "441                       Agricultural news                   328    0.792194  \n",
       "442                       Agricultural news                   365    0.799084  \n",
       "443                       Agricultural news                   442    0.801411  \n",
       "444                       Agricultural news                   420    0.793920  \n",
       "445                       Agricultural news                   361    0.787304  \n",
       "446                       Agricultural news                   427    0.789521  \n",
       "447                       Agricultural news                   371    0.854731  \n",
       "448            Tennessee farm and home news                   140    0.668110  \n",
       "449                     Tennessee farm news                   153    0.796328  \n",
       "450                                     NaN                     9   -1.000000  \n",
       "\n",
       "[451 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = data_dir_path.joinpath('TnFarmNews_title.csv')\n",
    "if csv_path.is_file():\n",
    "    processed_df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    processed_df = pd.DataFrame()\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ec5a7afa949cf9ae463491de389fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Images to process'), IntProgress(value=0, max=3451)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# batch process\n",
    "rows_list = []\n",
    "\n",
    "images_to_process_before_saving_csv = 100\n",
    "\n",
    "# uncomment to reset processed_df\n",
    "# processed_df = pd.DataFrame()\n",
    "\n",
    "# let's process the pages in random order\n",
    "# paths_list = page_1_paths_list\n",
    "paths_list = random.sample(page_1_paths_list, len(page_1_paths_list))\n",
    "\n",
    "number_of_paths = len(paths_list)\n",
    "\n",
    "# progress bar\n",
    "progress_label = Label('Images to process')\n",
    "progress_bar = IntProgress(min=0, max=number_of_paths)\n",
    "progress_widget = VBox([progress_label, progress_bar])\n",
    "display(progress_widget)\n",
    "\n",
    "\n",
    "for index, image_path in enumerate(paths_list, start=1):\n",
    "\n",
    "    # if the image_path is already in the dataframe skip it\n",
    "    if len(processed_df) > 0:\n",
    "        if (processed_df['path'] == str(image_path)).any():\n",
    "            continue\n",
    "\n",
    "    label = f'Processing {image_path.name} . . . {index}/{number_of_paths}'\n",
    "    progress_label.value = label\n",
    "\n",
    "    issue = TnFarmNews(image_path)\n",
    "    # issue.ocr()\n",
    "    # draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    issue.guess_title(title_crops_dict, debug=False)\n",
    "    # print(f'\\t{issue.BEST_TITLE}')\n",
    "\n",
    "    # get input row in dictionary format\n",
    "    # key = column_name\n",
    "    results_dictionary = {'path': str(issue.image_path),\n",
    "                          'title_guess': issue.BEST_TITLE,\n",
    "                          'title_crop': issue.BEST_TITLE_CROP,\n",
    "                          'title_matches_number': issue.MOST_MATCHES,\n",
    "                          'title_ssim': issue.BEST_SSIM_WITH_TITLE\n",
    "                          }\n",
    "\n",
    "    for key in results_dictionary:\n",
    "        if results_dictionary[key]:\n",
    "            continue\n",
    "        else:\n",
    "            results_dictionary.update({key: None})\n",
    "\n",
    "    rows_list.append(results_dictionary)\n",
    "\n",
    "    progress_bar.value = index\n",
    "    \n",
    "    # if modulo of processed images is 0 or it's the last image save data to the CSV \n",
    "    if index % images_to_process_before_saving_csv == 0 or index == number_of_paths:\n",
    "        \n",
    "        print(f'Saving data from {index} images to {csv_path}')\n",
    "        \n",
    "        # get dataframe from processed rows\n",
    "        crop_df = pd.DataFrame(rows_list)\n",
    "        \n",
    "        # add dataframes together\n",
    "        processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "        # drop duplicates\n",
    "        processed_df.drop_duplicates(subset=['path'], inplace=True)\n",
    "\n",
    "        # sort on image_path and reset the index\n",
    "        processed_df = processed_df.sort_values(by='path').reset_index(drop=True)\n",
    "        processed_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # reset rows_list\n",
    "        rows_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>title_crop</th>\n",
       "      <th>title_guess</th>\n",
       "      <th>title_matches_number</th>\n",
       "      <th>title_ssim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/images/0012_004266_001032_0001.tif</td>\n",
       "      <td>[1394, 72, 3195, 219]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>65</td>\n",
       "      <td>0.616356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/images/0012_004266_000725_0001.tif</td>\n",
       "      <td>[896, 564, 2302, 704]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>151</td>\n",
       "      <td>0.767425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/images/0012_004266_002492_0001.tif</td>\n",
       "      <td>[1399, 431, 2650, 810]</td>\n",
       "      <td>Farm news</td>\n",
       "      <td>79</td>\n",
       "      <td>0.776929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/images/0012_004266_000566_0001.tif</td>\n",
       "      <td>[965, 484, 2367, 617]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>123</td>\n",
       "      <td>0.755774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/images/0012_004266_002650_0001.tif</td>\n",
       "      <td>[1128, 5, 3187, 839]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>309</td>\n",
       "      <td>0.769517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/images/0012_004266_002321_0001.tif</td>\n",
       "      <td>[1377, 532, 2619, 912]</td>\n",
       "      <td>Farm news</td>\n",
       "      <td>95</td>\n",
       "      <td>0.752850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/images/0012_004266_002319_0001.tif</td>\n",
       "      <td>[1361, 533, 2604, 913]</td>\n",
       "      <td>Farm news</td>\n",
       "      <td>78</td>\n",
       "      <td>0.739334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/images/0012_004266_002382_0001.tif</td>\n",
       "      <td>[1370, 515, 2617, 889]</td>\n",
       "      <td>Farm news</td>\n",
       "      <td>96</td>\n",
       "      <td>0.771131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/images/0012_004266_003320_0001.tif</td>\n",
       "      <td>[1175, 23, 3219, 827]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>424</td>\n",
       "      <td>0.775449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/images/0012_004266_000081_0001.tif</td>\n",
       "      <td>[771, 442, 2235, 603]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>88</td>\n",
       "      <td>0.722951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/images/0012_004266_003056_0001.tif</td>\n",
       "      <td>[541, 523, 1745, 1230]</td>\n",
       "      <td>Agricultural and home economics packet</td>\n",
       "      <td>322</td>\n",
       "      <td>0.656652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/images/0012_004266_003026_0001.tif</td>\n",
       "      <td>[197, 456, 3014, 1085]</td>\n",
       "      <td>Agricultural &amp; home economics news</td>\n",
       "      <td>179</td>\n",
       "      <td>0.464845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/images/0012_004266_003440_0001.tif</td>\n",
       "      <td>[1098, 7, 3148, 823]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>427</td>\n",
       "      <td>0.789521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/images/0012_004266_000500_0001.tif</td>\n",
       "      <td>[914, 497, 2319, 621]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>126</td>\n",
       "      <td>0.822184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/images/0012_004266_001250_0001.tif</td>\n",
       "      <td>[1401, 316, 3227, 477]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>75</td>\n",
       "      <td>0.544718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/images/0012_004266_002761_0001.tif</td>\n",
       "      <td>[1093, 43, 3144, 891]</td>\n",
       "      <td>Agricultural &amp; home economics news</td>\n",
       "      <td>431</td>\n",
       "      <td>0.797794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/images/0012_004266_000194_0001.tif</td>\n",
       "      <td>[787, 371, 2194, 506]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>69</td>\n",
       "      <td>0.752399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/images/0012_004266_003225_0001.tif</td>\n",
       "      <td>[1132, 17, 3194, 842]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>406</td>\n",
       "      <td>0.744063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/images/0012_004266_001367_0001.tif</td>\n",
       "      <td>[1455, 144, 3216, 292]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>26</td>\n",
       "      <td>0.445690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/images/0012_004266_001651_0001.tif</td>\n",
       "      <td>[1396, 13, 3147, 196]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>145</td>\n",
       "      <td>0.683588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/images/0012_004266_000592_0001.tif</td>\n",
       "      <td>[666, 520, 2398, 678]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>18</td>\n",
       "      <td>0.524562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/images/0012_004266_002258_0001.tif</td>\n",
       "      <td>[1363, 506, 2620, 887]</td>\n",
       "      <td>Farm news</td>\n",
       "      <td>64</td>\n",
       "      <td>0.760186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>data/images/0012_004266_002931_0001.tif</td>\n",
       "      <td>[247, 457, 3048, 1087]</td>\n",
       "      <td>Agricultural &amp; home economics news</td>\n",
       "      <td>201</td>\n",
       "      <td>0.495227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>data/images/0012_004266_003444_0001.tif</td>\n",
       "      <td>[1133, 29, 3177, 837]</td>\n",
       "      <td>Agricultural news</td>\n",
       "      <td>371</td>\n",
       "      <td>0.854731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>data/images/0012_004266_000892_0001.tif</td>\n",
       "      <td>[1335, 117, 3107, 258]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>79</td>\n",
       "      <td>0.576688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>data/images/0012_004266_002005_0001.tif</td>\n",
       "      <td>[1320, 201, 3084, 362]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>111</td>\n",
       "      <td>0.661184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>data/images/0012_004266_001004_0001.tif</td>\n",
       "      <td>[1352, 99, 3124, 251]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>73</td>\n",
       "      <td>0.673474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>data/images/0012_004266_000403_0001.tif</td>\n",
       "      <td>[640, 520, 2287, 663]</td>\n",
       "      <td>Tennessee farm news</td>\n",
       "      <td>5</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>data/images/0012_004266_002214_0001.tif</td>\n",
       "      <td>[1328, 198, 3086, 385]</td>\n",
       "      <td>Tennessee farm and home news</td>\n",
       "      <td>157</td>\n",
       "      <td>0.820098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path              title_crop  \\\n",
       "0   data/images/0012_004266_001032_0001.tif   [1394, 72, 3195, 219]   \n",
       "1   data/images/0012_004266_000725_0001.tif   [896, 564, 2302, 704]   \n",
       "2   data/images/0012_004266_002492_0001.tif  [1399, 431, 2650, 810]   \n",
       "3   data/images/0012_004266_000566_0001.tif   [965, 484, 2367, 617]   \n",
       "4   data/images/0012_004266_002650_0001.tif    [1128, 5, 3187, 839]   \n",
       "5   data/images/0012_004266_002321_0001.tif  [1377, 532, 2619, 912]   \n",
       "6   data/images/0012_004266_002319_0001.tif  [1361, 533, 2604, 913]   \n",
       "7   data/images/0012_004266_002382_0001.tif  [1370, 515, 2617, 889]   \n",
       "8   data/images/0012_004266_003320_0001.tif   [1175, 23, 3219, 827]   \n",
       "9   data/images/0012_004266_000081_0001.tif   [771, 442, 2235, 603]   \n",
       "10  data/images/0012_004266_003056_0001.tif  [541, 523, 1745, 1230]   \n",
       "11  data/images/0012_004266_003026_0001.tif  [197, 456, 3014, 1085]   \n",
       "12  data/images/0012_004266_003440_0001.tif    [1098, 7, 3148, 823]   \n",
       "13  data/images/0012_004266_000500_0001.tif   [914, 497, 2319, 621]   \n",
       "14  data/images/0012_004266_001250_0001.tif  [1401, 316, 3227, 477]   \n",
       "15  data/images/0012_004266_002761_0001.tif   [1093, 43, 3144, 891]   \n",
       "16  data/images/0012_004266_000194_0001.tif   [787, 371, 2194, 506]   \n",
       "17  data/images/0012_004266_003225_0001.tif   [1132, 17, 3194, 842]   \n",
       "18  data/images/0012_004266_001367_0001.tif  [1455, 144, 3216, 292]   \n",
       "19  data/images/0012_004266_001651_0001.tif   [1396, 13, 3147, 196]   \n",
       "20  data/images/0012_004266_000592_0001.tif   [666, 520, 2398, 678]   \n",
       "21  data/images/0012_004266_002258_0001.tif  [1363, 506, 2620, 887]   \n",
       "22  data/images/0012_004266_002931_0001.tif  [247, 457, 3048, 1087]   \n",
       "23  data/images/0012_004266_003444_0001.tif   [1133, 29, 3177, 837]   \n",
       "24  data/images/0012_004266_000892_0001.tif  [1335, 117, 3107, 258]   \n",
       "25  data/images/0012_004266_002005_0001.tif  [1320, 201, 3084, 362]   \n",
       "26  data/images/0012_004266_001004_0001.tif   [1352, 99, 3124, 251]   \n",
       "27  data/images/0012_004266_000403_0001.tif   [640, 520, 2287, 663]   \n",
       "28  data/images/0012_004266_002214_0001.tif  [1328, 198, 3086, 385]   \n",
       "\n",
       "                               title_guess  title_matches_number  title_ssim  \n",
       "0             Tennessee farm and home news                    65    0.616356  \n",
       "1                      Tennessee farm news                   151    0.767425  \n",
       "2                                Farm news                    79    0.776929  \n",
       "3                      Tennessee farm news                   123    0.755774  \n",
       "4                        Agricultural news                   309    0.769517  \n",
       "5                                Farm news                    95    0.752850  \n",
       "6                                Farm news                    78    0.739334  \n",
       "7                                Farm news                    96    0.771131  \n",
       "8                        Agricultural news                   424    0.775449  \n",
       "9                      Tennessee farm news                    88    0.722951  \n",
       "10  Agricultural and home economics packet                   322    0.656652  \n",
       "11      Agricultural & home economics news                   179    0.464845  \n",
       "12                       Agricultural news                   427    0.789521  \n",
       "13                     Tennessee farm news                   126    0.822184  \n",
       "14            Tennessee farm and home news                    75    0.544718  \n",
       "15      Agricultural & home economics news                   431    0.797794  \n",
       "16                     Tennessee farm news                    69    0.752399  \n",
       "17                       Agricultural news                   406    0.744063  \n",
       "18            Tennessee farm and home news                    26    0.445690  \n",
       "19            Tennessee farm and home news                   145    0.683588  \n",
       "20                     Tennessee farm news                    18    0.524562  \n",
       "21                               Farm news                    64    0.760186  \n",
       "22      Agricultural & home economics news                   201    0.495227  \n",
       "23                       Agricultural news                   371    0.854731  \n",
       "24            Tennessee farm and home news                    79    0.576688  \n",
       "25            Tennessee farm and home news                   111    0.661184  \n",
       "26            Tennessee farm and home news                    73    0.673474  \n",
       "27                     Tennessee farm news                     5    0.611111  \n",
       "28            Tennessee farm and home news                   157    0.820098  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dataframe from processed rows\n",
    "crop_df = pd.DataFrame(rows_list)\n",
    "crop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataframes together\n",
    "processed_df = pd.concat([processed_df, crop_df])\n",
    "\n",
    "# drop duplicates\n",
    "processed_df.drop_duplicates(subset=['path'], inplace=True)\n",
    "\n",
    "# sort on image_path and reset the index\n",
    "processed_df = processed_df.sort_values(by='path').reset_index(drop=True)\n",
    "processed_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# based on low match data in the cell below, I changed the minimum matches to 10 to see if I could correct\n",
    "# the false positive on data/images/0012_004266_000395_0001.tif\n",
    "# I ended up dropping the values that were under 8 matches in the processed_df and saving out to CSV\n",
    "print(f'before drop: {len(processed_df)}')\n",
    "no_low_matches_df = processed_df[processed_df['title_matches_number'] >= 10].reset_index(drop=True)\n",
    "print(f'after drop: {len(no_low_matches_df)}')\n",
    "\n",
    "# processed_df = no_low_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string = 'string'\n",
    "isinstance(string, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(processed_df)):\n",
    "    path = processed_df.iloc[i]['path']\n",
    "    title = processed_df.iloc[i]['title_guess']\n",
    "    matches = processed_df.iloc[i]['title_matches_number']\n",
    "    crop_box = processed_df.iloc[i]['title_crop']\n",
    "    print(matches, path)\n",
    "    print(title)\n",
    "    if isinstance(crop_box, str):\n",
    "        crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "        points = crop_box.split(', ')\n",
    "        # print(points)\n",
    "        points = [int(x) for x in points]\n",
    "    elif isinstance(crop_box, list):\n",
    "        # print(f'crop_box: {crop_box}')\n",
    "        points = crop_box\n",
    "    else:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = points\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "low_matches_df = processed_df[processed_df['title_matches_number'] < 10].reset_index(drop=True)\n",
    "for i in range(len(low_matches_df)):\n",
    "    path = low_matches_df.iloc[i]['path']\n",
    "    title = low_matches_df.iloc[i]['title_guess']\n",
    "    matches = low_matches_df.iloc[i]['title_matches_number']\n",
    "    crop_box = low_matches_df.iloc[i]['title_crop']\n",
    "    # print(low_matches_df.iloc[i]['title_crop'])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # print(low_matches_df.iloc[i])\n",
    "    print(title, matches)\n",
    "    print(path)\n",
    "    print(type(crop_box))\n",
    "    print(crop_box)\n",
    "    print('')\n",
    "    if isinstance(crop_box, str):\n",
    "        print('string')\n",
    "        crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "        points = crop_box.split(', ')\n",
    "        # print(points)\n",
    "        points = [int(x) for x in points]\n",
    "    elif isinstance(crop_box, list):\n",
    "        print(f'crop_box: {crop_box}')\n",
    "        points = crop_box\n",
    "    else:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = points\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_matches_df = test[test['title_matches_number'] < 10].reset_index(drop=True)\n",
    "for i in range(len(low_matches_df)):\n",
    "    path = low_matches_df.iloc[i]['path']\n",
    "    title = low_matches_df.iloc[i]['title_guess']\n",
    "    matches = low_matches_df.iloc[i]['title_matches_number']\n",
    "    # print(low_matches_df.iloc[i]['title_crop'])\n",
    "\n",
    "    crop_box = low_matches_df.iloc[i]['title_crop']\n",
    "    crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "    points = crop_box.split(', ')\n",
    "    # print(points)\n",
    "    points = [int(x) for x in points]\n",
    "    x1, y1, x2, y2 = points\n",
    "\n",
    "    # print(low_matches_df.iloc[i])\n",
    "    print(title, matches)\n",
    "    if matches < 8:\n",
    "        print(path)\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_ssim_df = test[test['title_ssim'] < 0.5].reset_index(drop=True)\n",
    "for i in range(len(low_ssim_df)):\n",
    "    path = low_ssim_df.iloc[i]['path']\n",
    "    # print(low_ssim_df.iloc[i]['title_crop'])\n",
    "    title = low_ssim_df.iloc[i]['title_guess']\n",
    "    matches = low_ssim_df.iloc[i]['title_matches_number']\n",
    "    ssim = low_ssim_df.iloc[i]['title_ssim']\n",
    "    \n",
    "    crop_box = low_ssim_df.iloc[i]['title_crop']\n",
    "    crop_box = crop_box.replace('[', '').replace(']', '')\n",
    "    points = crop_box.split(', ')\n",
    "    # print(points)\n",
    "    points = [int(x) for x in points]\n",
    "    x1, y1, x2, y2 = points\n",
    "    \n",
    "    # print(low_ssim_df.iloc[i])\n",
    "    print(title, ssim)\n",
    "    image = cv2.imread(path)\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    quick_imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(title_csv_path)\n",
    "test[test['title_matches_number'] < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = crop_df['image_path'][0]\n",
    "image = cv2.imread(str(image_path))\n",
    "x1, y1, x2, y2 = crop_df['title_crop'][0]\n",
    "image_cropped = image[y1:y2, x1:x2]\n",
    "plt.imshow(image), plt.show()\n",
    "plt.imshow(image_cropped), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_images(file=page_1_paths_list):\n",
    "    # load image\n",
    "    issue = TnFarmNews(file)\n",
    "    issue.ocr()\n",
    "    # draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    # image = Image.open(file)\n",
    "    # temp_image_path = Path('_temp_image.jpg')\n",
    "    # image.save(temp_image_path)\n",
    "    # display(ipyImage(temp_image_path))\n",
    "    issue.guess_title(title_crops_dict, debug=True)\n",
    "    print(issue.best_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# this is my attempt to try and make a crop-value widget!\n",
    "\n",
    "page_1_paths_dictionary = {path.name: path for path in page_1_paths_list}\n",
    "initial_path = page_1_paths_list[0]\n",
    "initial_images_key = initial_path.name\n",
    "initial_image = cv2.imread(image)\n",
    "initial_x2 =\n",
    "\n",
    "# Create widgets\n",
    "images = widgets.Dropdown(\n",
    "    options=page_1_paths_dictionary, value=initial_path_key)\n",
    "crop_x1 = widgets.IntRangeSlider(value=(100, , min=0, max=initial_image.shape[1])\n",
    "crop_y1=widgets.IntSlider(value=0, min=0, max=initial_image.shape[0])\n",
    "crop_x2=widgets.IntSlider()\n",
    "\n",
    "# Updates the image options based on directory value\n",
    "def update_crop(*args):\n",
    "    crop_x1.max=os.listdir(directory.value)\n",
    "\n",
    "# Tie the image options to directory value\n",
    "directory.observe(update_crop, 'value')\n",
    "\n",
    "# Show the images\n",
    "def show_images(fdir, file):\n",
    "    display(Image(f'{fdir}/{file}'))\n",
    "\n",
    "_=interact(show_images, fdir=directory, file=images)\n",
    "def crop_image(file=page_1_paths_list, x1=(0, Image.open(file).size[0])):\n",
    "    # load image\n",
    "    issue=TnFarmNews(file)\n",
    "    issue.ocr()\n",
    "    draw_month_box(issue.line_and_word_boxes, issue.image_cropped)\n",
    "    # image = Image.open(file)\n",
    "    # temp_image_path = Path('_temp_image.jpg')\n",
    "    # image.save(temp_image_path)\n",
    "    # display(ipyImage(temp_image_path))\n",
    "    issue.guess_title(debug=True)\n",
    "    print(issue.best_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
